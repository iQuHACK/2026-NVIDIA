{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b1f70ff2-492a-41a4-97db-5da6d5775cb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SPDX-License-Identifier: Apache-2.0 AND CC-BY-NC-4.0\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa3992e-c095-4bd7-9f14-60abd0740b64",
      "metadata": {},
      "source": [
        "# Quantum Enhanced Optimization for Radar and Communications Applications \n",
        "\n",
        "\n",
        "The Low Autocorrelation Binary Sequences (LABS) is an important and challenging optimization problem with applications related to radar, telecommunications, and other signal related applications. This CUDA-Q Academic module will focus on a clever quantum-enhanced hybrid method developed in a collaboration between Kipu Quantum, University of the Basque Country EHU, and NVIDIA for solving the LABS problem. (This notebook was jointly developed with input from all collaborators.)\n",
        "\n",
        "Other CUDA-Q Academic modules like [Divide and Conquer MaxCut QAOA](https://github.com/NVIDIA/cuda-q-academic/tree/main/qaoa-for-max-cut) and [Quantum Finance](https://github.com/NVIDIA/cuda-q-academic/blob/main/quantum-applications-to-finance/03_qchop.ipynb), demonstrate how quantum computing can be used outright to solve optimization problems. This notebook demonstrates a slightly different approach. Rather than considering QPUs as the tool to produce the final answer, it demonstrates how quantum can be used to enhance the effectiveness of leading classical methods.  \n",
        "\n",
        "The benefits of such an approach were highlighted in [Scaling advantage with quantum-enhanced memetic tabu search for LABS](https://arxiv.org/html/2511.04553v1).  This notebook, co-created with the authors of the paper, will allow you to explore the findings of their research and write your own CUDA-Q code that builds a representative quantum-enhanced workflow for solving the LABS problem. Moreover, it will introduce advancements in counteradiabatic optimization techniques on which reduce the quantum resources required to run on a QPU.\n",
        "\n",
        "**Prerequisites:** This lab assumes you have a basic knowledge of quantum computing, including operators, gates, etc.  For a refresher on some of these topics, explore the [Quick start to Quantum](https://github.com/NVIDIA/cuda-q-academic/tree/main/quick-start-to-quantum) series.\n",
        "\n",
        "**In this lab you will:**\n",
        "* 1. Understand the LABS problem and its relation ot radar and communication applications.\n",
        "* 2. Solve LABS classically with memetic tabu search and learn about the limitations of such methods.\n",
        "* 3. Code a couteradiabatic algorithm using CUDA-Q to produce approximate solutions to the LABS problem.\n",
        "* 4. Use the CUDA-Q results to seed your tabu search and understand the potential benefits of this approach.\n",
        "\n",
        "\n",
        "**Terminology you will use:**\n",
        "* Low autocorrelation of binary sequences (LABS)\n",
        "* counteradiabatic optimization\n",
        "* memetic-tabu search\n",
        "\n",
        "**CUDA-Q Syntax you will use:**\n",
        "* cudaq.sample()\n",
        "* @cudaq.kernel\n",
        "* ry(), rx(), rz(), x(), h() \n",
        "* x.ctrl()\n",
        "\n",
        "Run the code below to initialize the libraries you will need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62179c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q cudaq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bfc407dd-113d-485c-88db-7ddbad344ead",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cudaq\n",
        "import numpy as np\n",
        "from math import floor\n",
        "import auxiliary_files.labs_utils as utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e821a8-47b4-4e5b-a713-4e1babada01f",
      "metadata": {},
      "source": [
        "## The LABS problem and applications\n",
        "\n",
        "The **Low Autocorrelation Binary Sequences (LABS)** problem is fundamental to many applications, but originated with applications to radar. \n",
        "\n",
        "Consider a radar that monitors airport traffic.  The radar signal sent to detect incoming planes must have as much range as possible to ensure safe approaches are planned well in advance.  The range of a radar signal can be increased by sending a longer pulse.  However, in order to differentiate between multiple objects, pulses need to be short to provide high resolution. So, how do you handle situations where you need both?\n",
        "\n",
        "One solution is a technique called pulse compression.  The idea is to send a long signal, but vary the phase at regular intervals such that the resolution is increased. Generally, the initial signal will encode a binary sequence of phase shifts, where each interval corresponds to a signal with a 0 or 180 degree phase shift. \n",
        "\n",
        "The tricky part is selecting an optimal encoding sequence.  When the signal returns, it is fed into a matched filter with the hope that a singular sharp peak will appear, indicating clear detection.  The autocorrelation of the original signal, or how similar the signal is to itself,  determines if a single peak or a messier signal with sidelobes will be detected. A signal should have high autocorrelation when overlayed on top of itself, but low autocorrelation when shifted with a lag. \n",
        "\n",
        "Consider the image below.  The signal on the left has a crisp single peak while the single on the right produces many undesirable sidelobes which can inhibit clear detection.  \n",
        "\n",
        "<img src=\"images/quantum_enhanced_optimization_LABS/radar.png\" width=\"800\">\n",
        "\n",
        "\n",
        "So, how do you select a good signal?   This is where LABS comes in, defining these questions as a binary optimization problem. Given a binary sequence of length $N$, $(s_1 \\cdots s_N) \\in {\\pm 1}^N$, the goal is to minimize the following objective function.\n",
        "\n",
        "$$ E(s) = \\sum_{k=1}^{N-1} C_k^2 $$\n",
        "\n",
        "Where $C_k$ is defined as. \n",
        "\n",
        " $$C_k= \\sum_{i=1}^{N-k} s_is_{i+k}$$\n",
        "\n",
        "\n",
        "So, each $C_k$ computes how similar the original signal is to the shifted one for each offset value $k$.  To explore this more, try the interactive widget linked [here](https://nvidia.github.io/cuda-q-academic/interactive_widgets/labs_visualization.html).  See if you can select a very good and very poor sequence and see the difference for the case of $N=7$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fa6dff-0fee-4251-a006-76d6ad426116",
      "metadata": {},
      "source": [
        "## Classical Solution of the LABS problem\n",
        "\n",
        "The LABS problem is tricky to solve for a few reasons. First, the configuration space grows exponentially.  Second, underlying symmetries of the problem result in many degeneracies in the optimization landscape severely inhibiting local search methods. \n",
        "\n",
        "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
        "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 1:</h3>\n",
        "    <p style=\"font-size: 16px; color: #333;\">\n",
        "Using the widget above, try to find some of the symmetries for the LABS problem. That is, for a fixed bitstring length, can you find patterns to produce the same energy with different pulse patterns. \n",
        "</div>\n",
        "\n",
        "The best known performance for a classical optimization technique is Memetic Tabu search (MTS) which exhibits a scaling of $O(1.34^N)$.  The MTS algorithm is depicted below.  It begins with a randomly selected population of bitstrings and finds the best solution from them.  Then, a child is selected by sampling directly from or combining multiple bitstrings from the population.  The child is mutated with probability $p_{mutate}$ and then input to a tabu search, which performs a modified greedy local search starting from the child bitstring.  If the result is better than the best in the population, it is updated as the new leader and randomly replaces a  bitstring in the population.\n",
        "\n",
        "\n",
        "<img src=\"images/quantum_enhanced_optimization_LABS/mts_algorithm.png\" width=\"500\">\n",
        "\n",
        "Such an approach is fast, parallelizable, and allows for exploration with improved searching of the solution landscape.  \n",
        "\n",
        "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
        "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 2:</h3>\n",
        "    <p style=\"font-size: 16px; color: #333;\">\n",
        "Before exploring any quantum approach, get a sense for how MTS works by coding it yourself based generally on the figure above. Algorithms for the combine and mutate steps are provided below as used in the paper. You may need to research more specific details of the process, especially for how tabu search is performed. The MTS procedure should output and optimal bitstring and its energy.  Also, write a function to visualize the results including the energy distribution of the final population.\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"images/quantum_enhanced_optimization_LABS/combine_mutate.png\" width=\"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "341c9a3f-565c-49ab-9903-0aa9a508722c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_E: 3 best_s: [ 1  1  1 -1 -1  1 -1]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#TODO - Write code to perform MTS\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "def to_pm1(s: np.ndarray) -> np.ndarray:\n",
        "    s = np.asarray(s, dtype=int)\n",
        "    u = np.unique(s)\n",
        "    if set(u.tolist()) <= {0, 1}:\n",
        "        return 2 * s - 1\n",
        "    return s\n",
        "\n",
        "def labs_correlations(s_pm1: np.ndarray) -> np.ndarray:\n",
        "    s = to_pm1(s_pm1)\n",
        "    N = len(s)\n",
        "    C = np.zeros(N, dtype=int)  # C[0] unused\n",
        "    for k in range(1, N):\n",
        "        C[k] = int(np.dot(s[:-k], s[k:]))\n",
        "    return C\n",
        "\n",
        "def labs_energy(s_pm1: np.ndarray) -> int:\n",
        "    C = labs_correlations(s_pm1)\n",
        "    return int(np.dot(C[1:], C[1:]))\n",
        "\n",
        "def energy_from_C(C: np.ndarray) -> int:\n",
        "    return int(np.dot(C[1:], C[1:]))\n",
        "\n",
        "def delta_for_flip(i: int, s: np.ndarray, C: np.ndarray) -> Tuple[int, np.ndarray]:\n",
        "    # s is +/-1, C is correlations for s\n",
        "    N = len(s)\n",
        "    si = s[i]\n",
        "    dC = np.zeros_like(C)\n",
        "    for k in range(1, N):\n",
        "        acc = 0\n",
        "        if i + k < N:\n",
        "            acc += si * s[i + k]\n",
        "        if i - k >= 0:\n",
        "            acc += s[i - k] * si\n",
        "        if acc:\n",
        "            dC[k] = -2 * acc\n",
        "    dE = int(np.sum(2 * C[1:] * dC[1:] + dC[1:] * dC[1:]))\n",
        "    return dE, dC\n",
        "\n",
        "def tabu_search(s0: np.ndarray, max_iters: int = 300, tenure: int = 7) -> Tuple[np.ndarray, int]:\n",
        "    s = to_pm1(s0).copy()\n",
        "    C = labs_correlations(s)\n",
        "    E = energy_from_C(C)\n",
        "\n",
        "    best_s = s.copy()\n",
        "    best_E = E\n",
        "\n",
        "    tabu_until = np.zeros(len(s), dtype=int)\n",
        "\n",
        "    for it in range(1, max_iters + 1):\n",
        "        best_move_i = None\n",
        "        best_move_E = None\n",
        "        best_move_dC = None\n",
        "\n",
        "        for i in range(len(s)):\n",
        "            dE, dC = delta_for_flip(i, s, C)\n",
        "            cand_E = E + dE\n",
        "\n",
        "            is_tabu = tabu_until[i] > it\n",
        "            if is_tabu and cand_E >= best_E:  # aspiration: allow if it improves best_E\n",
        "                continue\n",
        "\n",
        "            if best_move_E is None or cand_E < best_move_E:\n",
        "                best_move_i = i\n",
        "                best_move_E = cand_E\n",
        "                best_move_dC = dC\n",
        "\n",
        "        if best_move_i is None:\n",
        "            break\n",
        "\n",
        "        i = best_move_i\n",
        "        s[i] *= -1\n",
        "        C += best_move_dC\n",
        "        E = best_move_E\n",
        "        tabu_until[i] = it + tenure\n",
        "\n",
        "        if E < best_E:\n",
        "            best_E = E\n",
        "            best_s = s.copy()\n",
        "\n",
        "    return best_s, int(best_E)\n",
        "\n",
        "def combine_uniform(parent_a: np.ndarray, parent_b: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n",
        "    a = to_pm1(parent_a)\n",
        "    b = to_pm1(parent_b)\n",
        "    mask = rng.random(len(a)) < 0.5\n",
        "    child = np.where(mask, a, b).astype(int)\n",
        "    return child\n",
        "\n",
        "def mutate(child: np.ndarray, rng: np.random.Generator, p_bit: float = 0.05) -> np.ndarray:\n",
        "    s = to_pm1(child).copy()\n",
        "    flip = rng.random(len(s)) < p_bit\n",
        "    if not flip.any():\n",
        "        flip[rng.integers(0, len(s))] = True\n",
        "    s[flip] *= -1\n",
        "    return s\n",
        "\n",
        "def tournament_select(pop: np.ndarray, energies: np.ndarray, rng: np.random.Generator, k: int = 3) -> np.ndarray:\n",
        "    idx = rng.integers(0, len(pop), size=k)\n",
        "    best = idx[np.argmin(energies[idx])]\n",
        "    return pop[best]\n",
        "\n",
        "def mts(\n",
        "    N: int,\n",
        "    pop_size: int = 30,\n",
        "    generations: int = 200,\n",
        "    p_mutate: float = 0.2,\n",
        "    p_bit: float = 0.05,\n",
        "    tabu_iters: int = 300,\n",
        "    tabu_tenure: int = 7,\n",
        "    seed: int = 0,\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    pop = rng.choice([-1, 1], size=(pop_size, N)).astype(int)\n",
        "    energies = np.array([labs_energy(ind) for ind in pop], dtype=int)\n",
        "\n",
        "    best_idx = int(np.argmin(energies))\n",
        "    best_s = pop[best_idx].copy()\n",
        "    best_E = int(energies[best_idx])\n",
        "\n",
        "    for _ in range(generations):\n",
        "        p1 = tournament_select(pop, energies, rng, k=3)\n",
        "        p2 = tournament_select(pop, energies, rng, k=3)\n",
        "\n",
        "        child = combine_uniform(p1, p2, rng)\n",
        "        if rng.random() < p_mutate:\n",
        "            child = mutate(child, rng, p_bit=p_bit)\n",
        "\n",
        "        improved, e_improved = tabu_search(child, max_iters=tabu_iters, tenure=tabu_tenure)\n",
        "\n",
        "        # replace random member (simple + works fine for the lab)\n",
        "        j = int(rng.integers(0, pop_size))\n",
        "        pop[j] = improved\n",
        "        energies[j] = e_improved\n",
        "\n",
        "        if e_improved < best_E:\n",
        "            best_E = e_improved\n",
        "            best_s = improved.copy()\n",
        "\n",
        "    return best_s, best_E, pop, energies\n",
        "\n",
        "def plot_population_energies(energies: np.ndarray):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(energies, bins=20)\n",
        "    plt.xlabel(\"LABS energy\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(\"Final population energy distribution\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage (you can change N/pop_size/generations):\n",
        "N = 7\n",
        "best_s, best_E, pop, energies = mts(N)\n",
        "print(\"best_E:\", best_E, \"best_s:\", best_s)\n",
        "plot_population_energies(energies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b8abe7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# %pip install -q matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da2352b-7836-4af5-a7d3-5050a83775f7",
      "metadata": {},
      "source": [
        "## Building a Quantum Enhanced Workflow\n",
        "\n",
        "Despite the effectiveness of MTS, it still exhibits exponential scaling  $O(1.34^N)$ behavior and becomes intractable for large $N$.  Quantum computing provides a potential alternative method for solving the LABS problem because the properties of entanglement, interference, and superpositon may allow for a better global search.  Recent demonstrations have even produced evidence that the quantum approximate optimization algorithm (QAOA) can be used to reduce the scaling of the LABS problem to $O(1.21^N)$ for $N$ between 28 and 40 with quantum minimum finding.\n",
        "\n",
        "However, current quantum hardware limitations restrict solution to problems of greater than about $N=20$, meaning that it will be some time before quantum approaches can outperform the classical state of the art. It should also be noted that standard QAOA can struggle with LABS and require many layers to converge the parameters if other tricks are not employed.\n",
        "\n",
        "The authors of [Scaling advantage with quantum-enhanced memetic tabu search for LABS](https://arxiv.org/html/2511.04553v1) cleverly explored an alternate path that combines quantum and classical approaches and might be able to provide a more near-term benefit.  Instead of expecting the quantum computer to solve the problem entirely, they asked how a quantum approach might enhance MTS.\n",
        "\n",
        "The basic idea is that a quantum optimization routine could run first and the resulting state be sampled to produce a better population for MTS. Many such heuristics for defining the initial population are possible, but the rest of this notebook will explore their methodology, help you to build the workflow yourself, and allow you to analyze the benefits of their approach.\n",
        "\n",
        "The first step of quantum enhanced MTS (QE-MTS) is to prepare a circuit with CUDA-Q that approximates the ground state of the Hamiltonian corresponding to the LABS problem. You could do this with any optimization algorithm such as QAOA or using an adiabatic approach.  (See the [Quantum Portfolio Optimization](https://github.com/NVIDIA/cuda-q-academic/blob/main/quantum-applications-to-finance/03_qchop.ipynb) CUDA-Q Academic lab for a detailed comparison of these two common approaches.)\n",
        "\n",
        "The authors of this work opted for an adiabatic approach (More on why later). Recall that the goal of an adiabatic optimization is to begin with a Hamiltonian that has an easily prepared ground state ($H_i$). Then, the adiabatic Hamiltonian $H_{ad}$ can be constructed as $H_{ad}(\\lambda) = (1-\\lambda)H_i +\\lambda H_f $, where $\\lambda$ is a function of time and $H_f$ is the Hamiltonian representing a qubit encoding of the LABS problem. \n",
        "\n",
        "$$H_f = 2 \\sum_{i=1}^{N-2} \\sigma_i^z \\sum_{k=1}^{\\lfloor \\frac{N-i}{2} \\rfloor} \\sigma_{i+k}^z \n",
        "+ 4 \\sum_{i=1}^{N-3} \\sigma_i^z \\sum_{t=1}^{\\lfloor \\frac{N-i-1}{2} \\rfloor} \\sum_{k=t+1}^{N-i-t} \\sigma_{i+t}^z \\sigma_{i+k}^z \\sigma_{i+k+t}^z$$\n",
        "\n",
        "The authors also selected $H_i = \\sum_i h^x_i \\sigma^x_i $ which has an easily prepared ground state of $\\ket{+}^{\\otimes N}$.\n",
        "\n",
        "The challenge for implementing the optimization procedure becomes selection of an operator that will quickly and accurately evolve to the ground state of $H_f$.  One approach is to use a so-called auxiliary countradiabatic (CD) term $H_{CD}$, which corrects diabatic transitions that jump out of the ground state during the evolution. The figure below demonstrates the benefit of using a CD correction.\n",
        "\n",
        "\n",
        "<img src=\"images/quantum_enhanced_optimization_LABS/counteradiabatic.png\" width=\"900\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "An operator called the adiabatic gauge potential $A_{\\lambda}$ is the ideal choice for the CD term as it suppresses all possible diabatic transitions, resulting in the following total system to evolve.\n",
        "\n",
        "$$ H(\\lambda) = H_{ad}(\\lambda) + \\lambda H_{CD} (\\lambda) $$\n",
        "\n",
        "$A(\\lambda)$ is derrived from $H_{ad}(\\lambda)$  (see paper for details) as it contains information about underlying physics of the problem. \n",
        "\n",
        "There is a problem though.  The $A(\\lambda)$ term cannot be efficiently expressed exactly and needs to be approximated.  It also turns out that in the so called impulse regime, where the adiabatic evolution is very fast, $H_{cd} (\\lambda)$ dominates $H_{ad}(\\lambda)$, meaning that the final implementation corresponds to the operator $H(\\lambda) = H^1_{cd}(\\lambda)$ where  $H^1_{cd}(\\lambda)$ is a first order approximation of $A(\\lambda)$ (see equation 7 in the paper).\n",
        "\n",
        "A final step is to use Trotterization to define the quantum circuit to apply $e^{-\\theta (t) i H_{cd}}$. The details for this derivation are shown in the appendix of the paper. and result from equation B3 is shown below.  \n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "U(0, T) = \\prod_{n=1}^{n_{\\text{trot}}} & \\left[ \\prod_{i=1}^{N-2} \\prod_{k=1}^{\\lfloor \\frac{N-i}{2} \\rfloor} R_{Y_i Z_{i+k}}\\big(4\\theta(n\\Delta t)h_i^x\\big) R_{Z_i Y_{i+k}}\\big(4\\theta(n\\Delta t)h_{i+k}^x\\big) \\right] \\\\\n",
        "& \\times \\prod_{i=1}^{N-3} \\prod_{t=1}^{\\lfloor \\frac{N-i-1}{2} \\rfloor} \\prod_{k=t+1}^{N-i-t} \\bigg( R_{Y_i Z_{i+t} Z_{i+k} Z_{i+k+t}}\\big(8\\theta(n\\Delta t)h_i^x\\big) \\\\\n",
        "& \\quad \\times R_{Z_i Y_{i+t} Z_{i+k} Z_{i+k+t}}\\big(8\\theta(n\\Delta t)h_{i+t}^x\\big) \\\\\n",
        "& \\quad \\times R_{Z_i Z_{i+t} Y_{i+k} Z_{i+k+t}}\\big(8\\theta(n\\Delta t)h_{i+k}^x\\big) \\\\\n",
        "& \\quad \\times R_{Z_i Z_{i+t} Z_{i+k} Y_{i+k+t}}\\big(8\\theta(n\\Delta t)h_{i+k+t}^x\\big) \\bigg)\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "It turns out that this implementation is more efficient than QAOA in terms of gate count. The authors calculated that for $N=67$, QAOA would require 1.4 million entangling gates while the CD approach derived here requires only 236 thousand entangling gates.\n",
        "\n",
        "\n",
        "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
        "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 3:</h3>\n",
        "    <p style=\"font-size: 16px; color: #333;\">\n",
        "At first glance, this equation might looks quite complicated. However, observe the structure and note two \"blocks\" of terms.  Can you spot them?  \n",
        "\n",
        "They are 2 qubit terms that look like $R_{YZ}(\\theta)$ or $R_{ZY}(\\theta)$.\n",
        "\n",
        "As well as 4 qubit terms that look like $R_{YZZZ}(\\theta)$, $R_{ZYZZ}(\\theta)$, $R_{ZZYZ}(\\theta)$, or $R_{ZZZY}(\\theta)$.\n",
        "\n",
        "Thankfully the authors derive a pair of circuit implementations for the two and four qubit terms respectively, shown in the figures below.\n",
        "\n",
        "Using CUDA-Q, write a kernel for each which will be used later to construct the full implementation.\n",
        "\n",
        "* Hint: Remember that the adjoint of a rotation gate is the same as rotating in the opposite direction. \n",
        "\n",
        "* Hint: You may also want to define a CUDA-Q kernel for an R$_{ZZ}$ gate.\n",
        "\n",
        "* Hint: Implementing a circuit from a paper is a great place where AI can help accelerate your work.  If you have access to a coding assistant, feel free to use it here.\n",
        "</div>\n",
        "\n",
        "<img src=\"images/quantum_enhanced_optimization_LABS/kernels.png\" width=\"1300\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c91bbaae-62a1-41e7-a285-af885627a942",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO  Write CUDA-Q kernels to apply the 2 and 4 qubit operators. \n",
        "import cudaq\n",
        "import numpy as np\n",
        "\n",
        "@cudaq.kernel\n",
        "def rzz(theta: float, q0: cudaq.qubit, q1: cudaq.qubit):\n",
        "    \"\"\"\n",
        "    Implements RZZ(theta) = exp(-i * theta/2 * Z⊗Z) using:\n",
        "      CX(q0->q1) · RZ(theta) on q1 · CX(q0->q1)\n",
        "    (This matches the usual convention RZ(theta)=exp(-i*theta/2*Z).)\n",
        "    \"\"\"\n",
        "    x.ctrl(q0, q1)\n",
        "    rz(theta, q1)\n",
        "    x.ctrl(q0, q1)\n",
        "\n",
        "@cudaq.kernel\n",
        "def rzzzz(theta: float, q0: cudaq.qubit, q1: cudaq.qubit, q2: cudaq.qubit, q3: cudaq.qubit):\n",
        "    \"\"\"\n",
        "    Implements exp(-i * theta/2 * Z⊗Z⊗Z⊗Z) by parity-computing onto q3.\n",
        "    \"\"\"\n",
        "    x.ctrl(q0, q3)\n",
        "    x.ctrl(q1, q3)\n",
        "    x.ctrl(q2, q3)\n",
        "    rz(theta, q3)\n",
        "    x.ctrl(q2, q3)\n",
        "    x.ctrl(q1, q3)\n",
        "    x.ctrl(q0, q3)\n",
        "\n",
        "@cudaq.kernel\n",
        "def two_qubit_block(theta: float, qi: cudaq.qubit, qj: cudaq.qubit):\n",
        "    \"\"\"\n",
        "    Block in Fig. 3: R_{Y_i Z_j}(theta) · R_{Z_i Y_j}(theta)\n",
        "    Implemented via basis changes on the qubit that carries Y.\n",
        "    \"\"\"\n",
        "    # R_{Y_i Z_j}(theta)\n",
        "    rx(np.pi / 2.0, qi)\n",
        "    rzz(theta, qi, qj)\n",
        "    rx(-np.pi / 2.0, qi)\n",
        "\n",
        "    # R_{Z_i Y_j}(theta)\n",
        "    rx(np.pi / 2.0, qj)\n",
        "    rzz(theta, qi, qj)\n",
        "    rx(-np.pi / 2.0, qj)\n",
        "\n",
        "@cudaq.kernel\n",
        "def four_qubit_block(theta: float, q0: cudaq.qubit, q1: cudaq.qubit, q2: cudaq.qubit, q3: cudaq.qubit):\n",
        "    \"\"\"\n",
        "    Block in Fig. 4:\n",
        "      R_{YZZZ}(theta) · R_{ZYZZ}(theta) · R_{ZZYZ}(theta) · R_{ZZZY}(theta)\n",
        "    Each term is implemented as:\n",
        "      (basis change on the 'Y' qubit) · RZZZZ(theta) · (undo basis change)\n",
        "    \"\"\"\n",
        "    # YZZZ\n",
        "    rx(np.pi / 2.0, q0)\n",
        "    rzzzz(theta, q0, q1, q2, q3)\n",
        "    rx(-np.pi / 2.0, q0)\n",
        "\n",
        "    # ZYZZ\n",
        "    rx(np.pi / 2.0, q1)\n",
        "    rzzzz(theta, q0, q1, q2, q3)\n",
        "    rx(-np.pi / 2.0, q1)\n",
        "\n",
        "    # ZZYZ\n",
        "    rx(np.pi / 2.0, q2)\n",
        "    rzzzz(theta, q0, q1, q2, q3)\n",
        "    rx(-np.pi / 2.0, q2)\n",
        "\n",
        "    # ZZZY\n",
        "    rx(np.pi / 2.0, q3)\n",
        "    rzzzz(theta, q0, q1, q2, q3)\n",
        "    rx(-np.pi / 2.0, q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f113f324-6f58-4b5e-ab93-339d70f88c46",
      "metadata": {},
      "source": [
        "There are a few additional items we need to consider before completing the final implementation of the entire circuit.  One simplification we can make is that for our problem the $h_i^x$ terms are all 1 and any $h_b^x$ terms are 0, and are only there for generalizations of this model. \n",
        "\n",
        "The remaining challenge is derivation of the angles that are used to apply each of the circuits you defined above. These are obtained from two terms $\\lambda(t)$ and $\\alpha(t)$.  \n",
        "\n",
        "\n",
        "The $\\lambda(t)$ defines an annealing schedule and is generally a Sin function which slowly \"turns on\" the problem Hamiltonian.  For computing our angles, we need the derivative of $\\lambda(t)$.\n",
        "\n",
        "The $\\alpha$ term is a bit trickier and is the solution to a set of differential equations which minimize the distance between $H^1_{CD}(\\lambda)$ and $A(\\lambda)$.  The result is \n",
        "\n",
        "$$\\alpha(t) = \\frac{-\\Gamma_1(t)}{\\Gamma_2(t)} $$\n",
        "\n",
        "Where $\\Gamma_1(t)$ and $\\Gamma_2(t)$ are defined in equations 16 and 17 of the paper and essentially depend on the structure of the optimization problem.  Curious learners can look at the functions in `labs_utils.py`  to see how these are computed, based on the problem size and specific time step in the Trotter process. \n",
        "\n",
        "\n",
        "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
        "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 4:</h3>\n",
        "    <p style=\"font-size: 16px; color: #333;\">\n",
        "The `compute_theta` function, called in the following cells, requires all indices of the two and four body terms. These will be used again in our main kernel to apply the respective gates.  Use the products in the formula below to finish the function in the cell below.  Save them as `G2` and `G4` where each is a list of lists of indices defining the two and four term interactions. As you are translating an equation to a set of loops, this is a great opportunity to use an AI coding assistant.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "U(0, T) = \\prod_{n=1}^{n_{\\text{trot}}} & \\left[ \\prod_{i=1}^{N-2} \\prod_{k=1}^{\\lfloor \\frac{N-i}{2} \\rfloor} R_{Y_i Z_{i+k}}\\big(4\\theta(n\\Delta t)h_i^x\\big) R_{Z_i Y_{i+k}}\\big(4\\theta(n\\Delta t)h_{i+k}^x\\big) \\right] \\\\\n",
        "& \\times \\prod_{i=1}^{N-3} \\prod_{t=1}^{\\lfloor \\frac{N-i-1}{2} \\rfloor} \\prod_{k=t+1}^{N-i-t} \\bigg( R_{Y_i Z_{i+t} Z_{i+k} Z_{i+k+t}}\\big(8\\theta(n\\Delta t)h_i^x\\big) \\\\\n",
        "& \\quad \\times R_{Z_i Y_{i+t} Z_{i+k} Z_{i+k+t}}\\big(8\\theta(n\\Delta t)h_{i+t}^x\\big) \\\\\n",
        "& \\quad \\times R_{Z_i Z_{i+t} Y_{i+k} Z_{i+k+t}}\\big(8\\theta(n\\Delta t)h_{i+k}^x\\big) \\\\\n",
        "& \\quad \\times R_{Z_i Z_{i+t} Z_{i+k} Y_{i+k+t}}\\big(8\\theta(n\\Delta t)h_{i+k+t}^x\\big) \\bigg)\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "abf34168-42f9-4dbf-86e1-99976232ad7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_interactions(N):\n",
        "    \"\"\"\n",
        "    Generates the interaction sets G2 and G4 based on the loop limits in Eq. 15.\n",
        "    Returns standard 0-based indices as lists of lists of ints.\n",
        "    \n",
        "    Args:\n",
        "        N (int): Sequence length.\n",
        "        \n",
        "    Returns:\n",
        "        G2: List of lists containing two body term indices\n",
        "        G4: List of lists containing four body term indices\n",
        "    \"\"\"\n",
        "    \n",
        "    G2 = []\n",
        "    G4 = []\n",
        "\n",
        "    # --- 2-body terms: i = 1..N-2, k = 1..floor((N-i)/2)\n",
        "    # 0-based: i0 = 0..N-3, k = 1..floor((N-i0-1)/2)\n",
        "    for i0 in range(0, N - 2):\n",
        "        k_max = (N - i0 - 1) // 2\n",
        "        for k in range(1, k_max + 1):\n",
        "            G2.append([i0, i0 + k])\n",
        "\n",
        "    # --- 4-body terms: i = 1..N-3, t = 1..floor((N-i-1)/2), k = t+1..N-i-t\n",
        "    # 0-based: i0 = 0..N-4,\n",
        "    #          t = 1..floor((N-i0-2)/2),\n",
        "    #          k = t+1..(N - i0 - t - 1)\n",
        "    for i0 in range(0, N - 3):\n",
        "        t_max = (N - i0 - 2) // 2\n",
        "        for t in range(1, t_max + 1):\n",
        "            k_upper = N - i0 - t - 1\n",
        "            for k in range(t + 1, k_upper + 1):\n",
        "                G4.append([i0, i0 + t, i0 + k, i0 + k + t])\n",
        "\n",
        "    return G2, G4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3450f200-b191-41f5-88d2-974052ee45ac",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
        "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 5:</h3>\n",
        "    <p style=\"font-size: 16px; color: #333;\">\n",
        "You are now ready to construct the entire circuit and run the counteradiabatic optimization procedure. The final kernel needs to apply Equation 15 for a specified total evolution time $T$ and the `n_steps` number of Trotter steps.  It must also take as input, the indices for the two and four body terms and the thetas to be applied each step, as these cannot be computed within a CUDA-Q kernel.\n",
        "\n",
        "The helper function `compute_theta` computes the theta parameters for you, using a few additional functions in accordance with the equations defined in the paper.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8c2e6d7d-03b2-482f-bc36-d572e6d4855a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{ 00000000010011110110:1 00000000100001000010:1 00000000100001110010:1 00000001000110111001:1 00000001001011111011:1 00000001010100010111:1 00000001100000010001:1 00000001100010111111:1 00000001101010100000:1 00000010000011010100:1 00000010111000001110:1 00000010111000110010:1 00000010111110011110:1 00000011010001001011:1 00000011010110101100:1 00000011011101011100:1 00000011100111011111:1 00000011101110001001:1 00000011110000100001:1 00000011110000100011:1 00000011110001001100:1 00000100000101100100:1 00000100010010111101:1 00000100100100101111:1 00000100111100111111:1 00000100111110001010:1 00000110010001111010:1 00000110010100110111:1 00000110011100110101:1 00000110100010111101:1 00000110101011010000:1 00000110101111100111:1 00000111011000111111:1 00000111011111011111:1 00000111101000111111:1 00001000000001110001:1 00001000000110111001:1 00001000001011100011:1 00001000010100101011:1 00001000011100010111:1 00001000110110110110:1 00001010000010001000:1 00001010000101000001:1 00001010100111010000:1 00001010101101010010:1 00001011000011010111:1 00001011000100010001:1 00001011001100001111:1 00001011010101011011:1 00001011011010101101:1 00001011110111000110:1 00001011111111011101:1 00001100000111000101:1 00001100110000000000:1 00001101001010010010:1 00001101010110011010:1 00001101011100001100:1 00001101101100001000:1 00001101111110010110:1 00001110110101010011:1 00001110111101010010:1 00001111000001010100:1 00010000010100011111:1 00010000011000100011:1 00010000110010101111:1 00010001000011001100:1 00010001010001101101:1 00010001011110000111:1 00010001110110001000:1 00010010000100000000:1 00010010001111010001:1 00010010010110001101:1 00010010100000011001:1 00010010101011011010:1 00010011101001110011:1 00010011101100111111:1 00010011110010110001:1 00010011110011101101:1 00010011110100001001:1 00010011110101000101:1 00010100000000101001:1 00010100001010101101:1 00010100111001010110:1 00010101010010011011:1 00010101010100110010:1 00010101101100101010:1 00010101110000011111:1 00010110001001001000:1 00010111010100010101:1 00011000001011111101:1 00011000001111110000:1 00011000011000010000:1 00011000011100000111:1 00011000100000010111:1 00011000110111000010:1 00011000111000011001:1 00011001000011010111:1 00011001000101011000:1 00011001000111100001:1 00011001100010010101:1 00011001100010110111:1 00011001101100100111:1 00011010100000100010:1 00011010100010110000:1 00011010110000111100:1 00011010110111100100:1 00011011001101110110:1 00011011010111110011:1 00011011101100110001:1 00011011110111000111:1 00011100000111001011:1 00011101011001000010:1 00011101100001110011:1 00011101110001000011:1 00011101110100000011:1 00011101110110110110:1 00011101111011101010:1 00011110010011010110:1 00011110010011011100:1 00011110011011110101:1 00011111010010111000:1 00011111101110101011:1 00011111111010000110:1 00011111111101101011:1 00100000000001010001:1 00100000011000011100:1 00100000011101000101:1 00100000100110100010:1 00100000100111001000:1 00100000101000111111:1 00100000110001000100:1 00100001000010000100:1 00100001010011000000:1 00100001010011010110:1 00100010000000111001:1 00100010100101111011:1 00100010110111000001:1 00100010111011111010:1 00100011000010011001:1 00100011101010101110:1 00100011101011110000:1 00100100101110010110:1 00100100111000000000:1 00100101001001110101:1 00100101011000011101:1 00100101101011100011:1 00100101110100011101:1 00100110011101111101:1 00100110100100101110:1 00100110101111001001:1 00100110110111111000:1 00100111001111011000:1 00100111010111010110:1 00100111110110001110:1 00100111111010100111:1 00101000001001100000:1 00101000010000000101:1 00101001001101011101:1 00101001010111001110:1 00101001011010110110:1 00101001101101000100:1 00101001101111000000:1 00101010001001011000:1 00101010100011010111:1 00101010101001110111:1 00101010110000101100:1 00101010111100110110:1 00101011000100000100:1 00101011110010111001:1 00101100001110111000:1 00101101000110001110:1 00101101001010101010:1 00101101011011100100:1 00101101101000000010:1 00101110011101101110:1 00101110100011100010:1 00101110111101100000:1 00101111101010010100:1 00101111111101011001:1 00110000000010010010:1 00110000010111111000:1 00110000011001000110:1 00110000011010000110:1 00110000111000010010:1 00110000111001101001:1 00110000111100100000:1 00110001011001100000:1 00110001100010110011:1 00110001101100001010:1 00110001111101000010:1 00110010000011101110:1 00110010000111101111:1 00110010001100101011:1 00110010100001000011:1 00110010101001110001:1 00110010110011001001:1 00110011001110001001:1 00110011010001110111:1 00110011010101000110:1 00110100000000011101:1 00110100001011101001:1 00110100010100011001:1 00110100010101100010:1 00110100100011100111:1 00110100100100000101:1 00110100101010011011:1 00110101000111101100:1 00110101010011011010:1 00110101100111100110:1 00110111000001111100:1 00110111000111110001:1 00110111001110001010:1 00110111101010011101:1 00110111101010101110:1 00111000011110011100:1 00111000100100110001:1 00111001001111111110:1 00111001010000010000:1 00111001100000011001:1 00111001101111001111:1 00111001111100100111:1 00111010000101111010:1 00111010010101111011:1 00111010101000010000:1 00111010101010100111:1 00111010111100110101:1 00111011000101001011:1 00111011000111011100:1 00111011101001111000:1 00111011110011010101:1 00111011111001100010:1 00111100001011100110:1 00111100011001011011:1 00111100101111101101:1 00111100110101111110:1 00111100110111111100:1 00111101000100111111:1 00111101000101011100:1 00111101100111111011:1 00111101110000001010:1 00111101110100100010:1 00111110000101011101:1 00111110100011100011:1 00111111001110100111:1 00111111111011110110:1 01000000001111101100:1 01000000100001101100:1 01000000111000110111:1 01000001000110010100:1 01000001010101010100:1 01000001101000010001:1 01000001101100101000:1 01000001101100111111:1 01000001111001000110:1 01000010010010000111:1 01000010010110000001:1 01000010010110010010:1 01000010011000001010:1 01000010011100111100:1 01000010100100100010:1 01000010100111111011:1 01000011011100011010:1 01000011110000100010:1 01000100000001010100:1 01000100100101101101:1 01000100101100011000:1 01000101101101011110:1 01000110001010101000:1 01000110010100001111:1 01000110011100111011:1 01000110101110011001:1 01000110111010000000:1 01000111001100011111:1 01000111101011100010:1 01000111111110100110:1 01001000100110110110:1 01001000101010110101:1 01001000101011001101:1 01001000101110011000:1 01001000111110001101:1 01001001001000010000:1 01001001001101110001:1 01001001001110101011:1 01001001101010111001:1 01001001110000000011:1 01001001111011001101:1 01001010001000100101:1 01001010001101110110:1 01001010011111111000:1 01001010101011100010:1 01001011011100010001:1 01001011100001101100:1 01001011100101101111:1 01001011101110001100:1 01001100110101110100:1 01001101000100001100:1 01001101011101001010:1 01001101110000110100:1 01001110010001110010:1 01001110010100101100:1 01001111000000001100:1 01001111101011010011:1 01001111111000101011:1 01001111111100100000:1 01010000000000011111:1 01010000001010011101:1 01010000010100110011:1 01010000100011111001:1 01010000101001011100:1 01010000111101011100:1 01010001001011010101:1 01010001010000000000:1 01010001010001111000:1 01010001100111001101:1 01010010001010101100:1 01010010100010010111:1 01010010111101110100:1 01010011010010100011:1 01010011111110101010:1 01010011111111010011:1 01010100001001110110:1 01010100101010001011:1 01010100110100100100:1 01010100110110110000:1 01010100111000001111:1 01010101000100000001:1 01010101000100101000:1 01010101101101101111:1 01010101101110111111:1 01010101110110011110:1 01010101111010001001:1 01010101111011110000:1 01010101111100011001:1 01010110100010101011:1 01010110100111000010:1 01010110110000010100:1 01010111010000100110:1 01010111011110110011:1 01010111011110111100:1 01011000011000100101:1 01011000100010101111:1 01011000100110000111:1 01011000101010001011:1 01011000110101100110:1 01011000110111110010:1 01011000111110011111:1 01011000111111010000:1 01011001001011111011:1 01011001001100001010:1 01011001001110001100:1 01011001100001101010:1 01011001100101101111:1 01011001101100111000:1 01011001110100001111:1 01011001110110100101:1 01011010000010100011:1 01011010000011110111:1 01011010000101010000:1 01011010001110101100:1 01011010011011110011:1 01011010100100001111:1 01011011010100000011:1 01011011010101101000:1 01011011011110011000:1 01011011100100110111:1 01011011111000011111:1 01011100011101100110:1 01011100101001100101:1 01011100101101101111:1 01011100111000011110:1 01011101010101100101:1 01011101111111011100:1 01011110000100101110:1 01011110010111010001:1 01011110101110100110:1 01011111001111011001:1 01011111011100110101:1 01011111100101100110:1 01100000000110010010:1 01100000001001000110:1 01100000010111001101:1 01100001010000001110:1 01100001011001010011:1 01100001011011111000:1 01100001111100100111:1 01100010001001001101:1 01100010011001110100:1 01100010100000111100:1 01100010111011100101:1 01100011010100011101:1 01100011010101000001:1 01100011111001100010:1 01100100100010110100:1 01100101001100101001:1 01100101011001100110:1 01100110010011110110:1 01100110010111111011:1 01100110101101000111:1 01100110110100110100:1 01100111000101100001:1 01101000000010010100:1 01101000001100110000:1 01101000010110001010:1 01101000010111011100:1 01101000011001101010:1 01101000100100111001:1 01101000100111100100:1 01101000101000110101:1 01101001000000110001:1 01101001000111110110:1 01101001010010010100:1 01101001010110111010:1 01101001011000101111:1 01101001011100011110:1 01101001011101110011:1 01101001110101000110:1 01101010100000001111:1 01101010101100111000:1 01101011000000000001:1 01101011000011110011:1 01101011001001000111:1 01101011101000000010:1 01101100000011011001:1 01101100001011000011:1 01101100001101010100:1 01101100100001010010:1 01101101010100100000:1 01101101011100001101:1 01101101100110010010:1 01101110001010100101:1 01101110010110100011:1 01101110100101000111:1 01101110100111001000:1 01101110110100110011:1 01101110111101110000:1 01101110111110100000:1 01101110111111011011:1 01101111001001111001:1 01101111001101100101:1 01101111010101111000:1 01110000000000110100:1 01110000010001000001:1 01110000110110100000:1 01110000111010110110:1 01110001001011001011:1 01110001010000001011:1 01110001011101111100:1 01110010000010101011:1 01110010000100000111:1 01110010000110011101:1 01110010000111000110:1 01110010001011100000:1 01110010001100011010:1 01110010010101110010:1 01110010011001001010:1 01110010111100000100:1 01110011010100101100:1 01110011100011010000:1 01110011100111010011:1 01110100110100001001:1 01110100110110100110:1 01110101010000000110:1 01110101011101110010:1 01110101100100010101:1 01110101111010110000:1 01110110000000011010:1 01110110000001000011:1 01110110001011101001:1 01110110010000111011:1 01110110011001110110:1 01110111010011110001:1 01111000010110110111:1 01111000110010110011:1 01111000111000011000:1 01111001000011100000:1 01111001100100000111:1 01111001101010000100:1 01111010010011111100:1 01111010100001110101:1 01111010100101000000:1 01111010111111110000:1 01111011000001000100:1 01111011000010011000:1 01111011001011001010:1 01111011010101110001:1 01111011110100110000:1 01111011111110011001:1 01111011111110101111:1 01111100000001110010:1 01111100011011110100:1 01111100100011011011:1 01111100101011010110:1 01111100101100000000:1 01111101101100101100:1 01111101111001001010:1 01111110011110011011:1 01111110110110111010:1 01111110111011000011:1 01111110111100011101:1 01111111011101000110:1 01111111100001101001:1 01111111101110111100:1 10000000011111100101:1 10000001001111000100:1 10000001101001000010:1 10000001110000100001:1 10000011011101010000:1 10000011101010000111:1 10000100011011000011:1 10000100100110000011:1 10000100101100100001:1 10000100111011110111:1 10000101100101001011:1 10000101101000100010:1 10000101101001000100:1 10000101101001100011:1 10000110000001001111:1 10000110001111011100:1 10000110010100100000:1 10000110011110010100:1 10000110011111101100:1 10000110100110111001:1 10000110101001100100:1 10000110111111011100:1 10000111000010110111:1 10000111001001010000:1 10000111001110000000:1 10000111010000011010:1 10000111010100011101:1 10000111101000101001:1 10000111101101000100:1 10001000000001110110:1 10001000001001011110:1 10001000001001110000:1 10001000001110000000:1 10001000011001100100:1 10001000011011110000:1 10001000101111110111:1 10001000110001001111:1 10001000110100110010:1 10001001011001011010:1 10001001111001101100:1 10001010010011011011:1 10001010011111101111:1 10001010100011010100:1 10001010100110100001:1 10001010101100110101:1 10001011001100100011:1 10001011100010101101:1 10001011101101010110:1 10001011110011101011:1 10001100011011000101:1 10001100110011101010:1 10001100110110100001:1 10001100111100111011:1 10001101001010000001:1 10001101010101011101:1 10001101101101111011:1 10001101111011111011:1 10001110000000110010:1 10001110100010101101:1 10001110101110110000:1 10001111001011111011:1 10001111100010100011:1 10001111101010100100:1 10001111101110001011:1 10001111110011011001:1 10010000000100110000:1 10010000001100111000:1 10010000010000110100:1 10010000101110101100:1 10010000110010100100:1 10010000111101011000:1 10010001101001011111:1 10010010011011000001:1 10010010110011011100:1 10010010110110010001:1 10010011111110101101:1 10010100000000011010:1 10010100001010111011:1 10010100010101011011:1 10010100101011000101:1 10010100110001101111:1 10010100110110101010:1 10010100111101011110:1 10010101000100000101:1 10010101000101010011:1 10010101010011000100:1 10010101110110100000:1 10010110001100011000:1 10010110010000111100:1 10010110011100111111:1 10010110011110011111:1 10010110110010100111:1 10010110111111101001:1 10010111001001100001:1 10010111001011111100:1 10010111011110000000:1 10010111101011010110:1 10011000000100000011:1 10011000010001101111:1 10011000100101100111:1 10011000100111111011:1 10011000101011100010:1 10011000110000101001:1 10011000110001111010:1 10011001011000111011:1 10011001011001111000:1 10011001011111101011:1 10011001100000000010:1 10011001101110100000:1 10011001111010001011:1 10011010100100011111:1 10011010100100101100:1 10011010110000111010:1 10011011000011100000:1 10011011011101010011:1 10011011100001101111:1 10011011110101011001:1 10011100010100101100:1 10011100010100110100:1 10011100011000010000:1 10011100011101100010:1 10011100011110010010:1 10011100101101101001:1 10011101001000010000:1 10011101001101111000:1 10011101100001100011:1 10011101101010010101:1 10011110011111010111:1 10011110101110110000:1 10011110101111011011:1 10011110110011011000:1 10011111001101101001:1 10011111010001000111:1 10011111011100001000:1 10100000000100010110:1 10100000000101100001:1 10100000011100111011:1 10100000101111000110:1 10100000110110100101:1 10100000111000101011:1 10100001010101100100:1 10100001111101000111:1 10100010110000011010:1 10100010110001111010:1 10100010110101111110:1 10100010110110011000:1 10100010111011001100:1 10100011010010111111:1 10100011011010001001:1 10100011110000010010:1 10100100110111110001:1 10100100111110111100:1 10100101100110011001:1 10100101110000101001:1 10100110000010000011:1 10100110100001110011:1 10100110101100111010:1 10100111000100000011:1 10100111011010101000:1 10100111101001011001:1 10100111111110011001:1 10101000000011110001:1 10101000001110011001:1 10101000110000111001:1 10101001011000011010:1 10101001100110011001:1 10101001101101110011:1 10101001101111101011:1 10101010100111000001:1 10101010110001111101:1 10101010110101010000:1 10101010111010111101:1 10101011001011000001:1 10101011011110010001:1 10101011110010100111:1 10101100000011000101:1 10101101010110001000:1 10101101011101110100:1 10101101101011100100:1 10101101111101101000:1 10101110001000110010:1 10101111101000100111:1 10101111111001110000:1 10101111111100010010:1 10110000000001100010:1 10110000000100111111:1 10110000011101111001:1 10110000111001011100:1 10110001000000010100:1 10110001000110001110:1 10110001000111001001:1 10110001001101110011:1 10110001011001010110:1 10110001100111001010:1 10110001111100111000:1 10110010010101001001:1 10110010100000001100:1 10110010111110000001:1 10110011000110110111:1 10110100010110111110:1 10110100011101100110:1 10110100100011110010:1 10110100101100110111:1 10110100110011000011:1 10110100111111100000:1 10110101000011100110:1 10110101110000011111:1 10110110011001001000:1 10110110100011111000:1 10110110111110111000:1 10110111000100010100:1 10110111110000010111:1 10110111111100010000:1 10111000000000100111:1 10111000011101010111:1 10111000100111110111:1 10111000110111101011:1 10111001010110111111:1 10111001101000010101:1 10111001101100100000:1 10111010001011000111:1 10111010001011010110:1 10111010001101011110:1 10111010010101010111:1 10111010101101100110:1 10111010110000001110:1 10111010110010011110:1 10111011001111110111:1 10111011011101101110:1 10111011101001100111:1 10111011101110100011:1 10111011110110011010:1 10111100000100110110:1 10111100001010001111:1 10111100010011000111:1 10111100010101111110:1 10111100011111000111:1 10111100100010011010:1 10111100100101010001:1 10111100110001110100:1 10111101011100000100:1 10111101101101000010:1 10111101110001001110:1 10111110010010011111:1 10111110100001000110:1 10111110110101101101:1 10111111000010010110:1 10111111000111001101:1 10111111111000011011:1 10111111111111000111:1 11000000000111101011:1 11000000001111100101:1 11000000010010111001:1 11000000101001001010:1 11000000101010100100:1 11000000101011000101:1 11000000110111110101:1 11000001000110001111:1 11000001010000100110:1 11000001010110100110:1 11000001101011001100:1 11000001101101110010:1 11000010100010001011:1 11000010101010000010:1 11000010110010001011:1 11000010111111011111:1 11000011001000100110:1 11000011010001001011:1 11000011111110100001:1 11000100110100011001:1 11000100111101001001:1 11000101000110110110:1 11000101000111011000:1 11000101001111101100:1 11000101110000100011:1 11000101111101010100:1 11000110011011111111:1 11000110100100111011:1 11000110110111000010:1 11000110111110001010:1 11000111101101000100:1 11000111110010100100:1 11001000001000011010:1 11001000010001011101:1 11001000010110111000:1 11001000110101010000:1 11001000111101101100:1 11001001001011001010:1 11001001011011111101:1 11001001101010010001:1 11001010011001001011:1 11001010101000010111:1 11001010110000000001:1 11001010110111101001:1 11001011011111100010:1 11001011100011100011:1 11001011111011000100:1 11001100011011100010:1 11001100011011101110:1 11001100011110101011:1 11001101001000100111:1 11001101111110010100:1 11001110110100110010:1 11001110111110010110:1 11001111000000100010:1 11001111010001011111:1 11001111010001110101:1 11001111100101001111:1 11001111110000000110:1 11001111110011001111:1 11010000000010100100:1 11010000010100100110:1 11010000101011111101:1 11010000110110011011:1 11010001000000101110:1 11010001001001101101:1 11010001001010110010:1 11010001010111100010:1 11010001100101101010:1 11010001101010111000:1 11010001110101001101:1 11010010110111001000:1 11010011001001011111:1 11010011110011001011:1 11010011111000001011:1 11010100000011110101:1 11010100000110110010:1 11010100001011010110:1 11010100100111011100:1 11010100100111101001:1 11010101001010110110:1 11010101010011100101:1 11010101011111110011:1 11010101100001000010:1 11010101100001111110:1 11010101100111000011:1 11010110100000110011:1 11010110101000010101:1 11010110110011001101:1 11010111001010001111:1 11011000000010101011:1 11011000010000010011:1 11011001001011010011:1 11011001100101001000:1 11011001101000000101:1 11011001111001001000:1 11011010000001110101:1 11011010010000100101:1 11011010010011010111:1 11011011000001011110:1 11011011101000101010:1 11011100000011110001:1 11011100000100011111:1 11011100011100101011:1 11011100100000010100:1 11011100100010001111:1 11011100101000111111:1 11011100111111001110:1 11011101101001100010:1 11011101110010100111:1 11011101110101000011:1 11011110001011000001:1 11011110010000111101:1 11011110010111100101:1 11011110100000110001:1 11011110101001101100:1 11011110101110011000:1 11011110110100101111:1 11011110111100101101:1 11011110111110000111:1 11011111010000100110:1 11011111100111101001:1 11011111111000110011:1 11011111111011100101:1 11011111111111100110:1 11100000001111010000:1 11100000011110101011:1 11100000100100101010:1 11100001001100010100:1 11100001011101101000:1 11100001100111001000:1 11100001101111001001:1 11100010011110011111:1 11100011010011010001:1 11100011100010011101:1 11100011101011000100:1 11100011111111101000:1 11100100010010010011:1 11100101000111111011:1 11100101010110010000:1 11100101101101010101:1 11100110011101110011:1 11100110100000110010:1 11100110100100111000:1 11100110101001101101:1 11100110110101010001:1 11100110110101110100:1 11100110111001010100:1 11100111011101100100:1 11100111111110011101:1 11101000000011100011:1 11101000010101100111:1 11101000100100010011:1 11101000111011110001:1 11101001001000011101:1 11101001010111010101:1 11101001011011011100:1 11101001100001100110:1 11101001101101100011:1 11101001110011000000:1 11101001110100110011:1 11101001111000000000:1 11101001111110011111:1 11101001111111010001:1 11101010100001101010:1 11101010100011010010:1 11101010100101111110:1 11101010101110011001:1 11101011000010111100:1 11101011010001101011:1 11101100111100011111:1 11101101000100101011:1 11101101010001000001:1 11101101101010001100:1 11101110001100111010:1 11101111000010101000:1 11101111100101010100:1 11101111101001101110:1 11101111110101100000:1 11110000000000011110:1 11110000001111111010:1 11110000110010011011:1 11110000110111111111:1 11110000111010011110:1 11110001001000010110:1 11110001001011111110:1 11110001011100111000:1 11110001100101000010:1 11110001101111000011:1 11110010001101000110:1 11110010010000110111:1 11110010011010010101:1 11110010100000000100:1 11110010110110111000:1 11110011001101111100:1 11110011100000111010:1 11110011100100000001:1 11110011100101100011:1 11110011101001010001:1 11110100001110000110:1 11110100010000101101:1 11110100011011000110:1 11110100100010010110:1 11110100110110110110:1 11110100111111011111:1 11110101000001001000:1 11110101101011110111:1 11110101101100110000:1 11110110001110101001:1 11110110010011111001:1 11110110100111100000:1 11110110101110110001:1 11110110111110111111:1 11110111000000101111:1 11110111011011001111:1 11110111011100110101:1 11110111100110000000:1 11110111101101000111:1 11110111101110011101:1 11111001001000011111:1 11111001010010110100:1 11111001010100010101:1 11111001011001000110:1 11111001100001101100:1 11111001101011011100:1 11111001101101100001:1 11111010001111101010:1 11111010011001001010:1 11111010011100010111:1 11111010100000011110:1 11111010101010110000:1 11111011000101111011:1 11111011011000101100:1 11111100010010010100:1 11111100100100111100:1 11111101000010101001:1 11111101010011001110:1 11111101010101100100:1 11111110010000010110:1 11111110011110011111:1 11111110101010100110:1 11111110110101001100:1 11111110110111110000:1 11111111000110011101:1 11111111010111101101:1 11111111011010100100:1 11111111100111100110:1 11111111101010001010:1 }\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@cudaq.kernel\n",
        "def trotterized_circuit(\n",
        "    N: int,\n",
        "    G2: list[list[int]],\n",
        "    G4: list[list[int]],\n",
        "    steps: int,\n",
        "    dt: float,   # (kept to match notebook signature; not needed if thetas are precomputed)\n",
        "    T: float,    # (same)\n",
        "    thetas: list[float],\n",
        "):\n",
        "    reg = cudaq.qvector(N)\n",
        "    h(reg)\n",
        "\n",
        "    # Apply Eq. 15 as a trotter product over steps\n",
        "    for n in range(steps):\n",
        "        theta = thetas[n]\n",
        "\n",
        "        # 2-qubit block: for each [i, j], apply R_{Y_i Z_j}(4θ) then R_{Z_i Y_j}(4θ)\n",
        "        for e in range(len(G2)):\n",
        "            i = G2[e][0]\n",
        "            j = G2[e][1]\n",
        "            two_qubit_block(4.0 * theta, reg[i], reg[j])\n",
        "\n",
        "        # 4-qubit block: for each [a,b,c,d], apply the 4 rotations with angle 8θ\n",
        "        for e in range(len(G4)):\n",
        "            a = G4[e][0]\n",
        "            b = G4[e][1]\n",
        "            c = G4[e][2]\n",
        "            d = G4[e][3]\n",
        "            four_qubit_block(8.0 * theta, reg[a], reg[b], reg[c], reg[d])\n",
        "\n",
        "# TODO - Sample your kernel to make sure it works\n",
        "T = 1\n",
        "n_steps = 1\n",
        "dt = T / n_steps\n",
        "N = 20\n",
        "\n",
        "G2, G4 = get_interactions(N)\n",
        "\n",
        "thetas = []\n",
        "for step in range(1, n_steps + 1):\n",
        "    t = step * dt\n",
        "    thetas.append(utils.compute_theta(t, dt, T, N, G2, G4))\n",
        "counts = cudaq.sample(trotterized_circuit, N, G2, G4, n_steps, dt, T, thetas)\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb89d90e-66e2-4700-85b9-40df9fca22c1",
      "metadata": {},
      "source": [
        "## Generating Quantum Enhanced Results\n",
        "\n",
        "Recall that the point of this lab is to demonstrate the potential benefits of running a quantum subroutine as a preprocessing step for classical optimization of a challenging problem like LABS. you now have all of the tools you need to try this for yourself.\n",
        "\n",
        "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
        "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 6:</h3>\n",
        "    <p style=\"font-size: 16px; color: #333;\">\n",
        "Use your CUDA-Q code to prepare an initial population for your memetic search algorithm and see if you can improve the results relative to a random initial population.  If you are running on a CPU, you will need to run smaller problem instances. The code below sets up the problem\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8e5f02e6-41bf-4634-9cb1-f0f543ef2e3f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QE-MTS best_E: 26\n",
            "Random MTS best_E: 26\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAGJCAYAAAAwtrGcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANrJJREFUeJzt3Xd4VGXexvF7EkgnoRhKNASkKlUQEJFes4hEWFkFaYIoVYy7QnZFurEgxEUUBQ26C8KqFMUFRDpIEUIRUWrArIAUIQFCAmSe9w/NvEwq5CSZgXw/1zXXxTznOef85pnDmTunzNiMMUYAAAB55OHqAgAAwK2NMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDCBAnP06FHZbDbNmTOnQNdTqVIl9evXr0DXUVjmzJkjm82mo0eP5tsyC+t9wI0ZN26cbDabU1thbcNZbQv9+vVTQEBAga87nc1m07hx4wptfSgchAnkWfoHX1aP0aNHu7q8ImfevHmKiYlxdRkoJP/973/d9kPZnWtDwSjm6gJw65swYYIqV67s1Fa7dm2FhYXp8uXLKl68uIsqK1rmzZunvXv3auTIkU7tvA/ub//+/fLwuLm/7f773/9qxowZN/WhXVjbQk61Xb58WcWK8dFzu+EdhWXh4eG6//77s5zm4+NTyNUgI5vNxvvwh+TkZPn5+bm6jEy8vb0LdPnXrl2T3W6Xl5eXy7cFV68fBYPTHCgwOZ2f/eWXXxQREaGAgAAFBwfrr3/9q9LS0pzmnzJlih588EGVKVNGvr6+atiwoT777DNLtUyZMkXTpk1TWFiYfH191bJlS+3duzdT/9WrV6t58+by9/dXyZIl1bVrV/34449OfdLPff/000/q0aOHAgMDVaZMGT333HNKSUnJcRzS3cj54yVLlqhz584KCQmRt7e3qlSpookTJzqNV6tWrfTVV1/p2LFjjlNNlSpVynH9N/MaDx06pH79+qlkyZIKCgpS//79lZycnGPd6bZu3apOnTopKChIfn5+atmypTZt2mRpPf/+97/VsGFD+fr6qnTp0nr88ceVkJDg1KdVq1aqXbu2duzYoRYtWsjPz09///vfJUlnz55V7969FRgYqJIlS6pv377avXu30zjFxsbKZrNp586dmdb/yiuvyNPTU7/88kuOr33jxo1q1KiRfHx8VKVKFb333ntZ9st4zcTVq1c1fvx4VatWTT4+PipTpoweeughrVy5UtLv/49mzJghSU6nFyXnbT0mJkZVqlSRt7e39u3bl+O2eOTIEXXs2FH+/v4KCQnRhAkTdP2PSq9du1Y2m01r1651mi/jMnOqLb0t4za/c+dOhYeHKzAwUAEBAWrbtq22bNni1Cf9tOqmTZsUGRmp4OBg+fv769FHH9Xp06ezfgNQaDgyAcsSExN15swZp7Y77rgj2/5paWnq2LGjmjRpoilTpuibb77Rm2++qSpVqmjw4MGOfm+99ZYeeeQR9erVS1euXNH8+fP12GOPaenSpercuXOeav3444914cIFDR06VCkpKXrrrbfUpk0bff/99ypXrpwk6ZtvvlF4eLjuvvtujRs3TpcvX9b06dPVrFkzxcXFOT6k0/Xo0UOVKlVSdHS0tmzZon/+8586d+6cPv744zzVmNGcOXMUEBCgyMhIBQQEaPXq1Xr55ZeVlJSkN954Q5L0j3/8Q4mJifrf//6nadOmSVKOF9Xl5TVWrlxZ0dHRiouL0+zZs1W2bFm99tprOda+evVqhYeHq2HDhho7dqw8PDwUGxurNm3aaMOGDWrcuPFNr2fy5MkaM2aMevTooYEDB+r06dOaPn26WrRooZ07d6pkyZKOvmfPnlV4eLgef/xxPfnkkypXrpzsdru6dOmibdu2afDgwapZs6aWLFmivn37OtXy5z//WUOHDtXcuXN13333OU2bO3euWrVqpTvvvDPb1/7999+rQ4cOCg4O1rhx43Tt2jWNHTvWsZ3lZNy4cYqOjtbAgQPVuHFjJSUlafv27YqLi1P79u31zDPP6Pjx41q5cqX+9a9/ZbmM2NhYpaSkaNCgQfL29lbp0qVlt9uz7JuWlqZOnTrpgQce0Ouvv67ly5dr7NixunbtmiZMmJBrvde7kdqu98MPP6h58+YKDAzUiy++qOLFi+u9995Tq1attG7dOjVp0sSp//Dhw1WqVCmNHTtWR48eVUxMjIYNG6YFCxbcVJ3IZwbIo9jYWCMpy4cxxsTHxxtJJjY21jFP3759jSQzYcIEp2Xdd999pmHDhk5tycnJTs+vXLliateubdq0aePUHhYWZvr27Ztjrem1+Pr6mv/973+O9q1btxpJ5vnnn3e01a9f35QtW9acPXvW0bZ7927j4eFh+vTp42gbO3askWQeeeQRp3UNGTLESDK7d+/OdhzSSTJjx451PE8f0/j4+GzHwRhjnnnmGePn52dSUlIcbZ07dzZhYWHZvvbr13+zr/Gpp55yWuajjz5qypQpk2ld17Pb7aZatWqmY8eOxm63O72eypUrm/bt29/0eo4ePWo8PT3N5MmTnfp9//33plixYk7tLVu2NJLMzJkznfp+/vnnRpKJiYlxtKWlpZk2bdpkGqcnnnjChISEmLS0NEdbXFxctu/n9SIiIoyPj485duyYo23fvn3G09PTZNz1ZtyG69WrZzp37pzj8ocOHZppOcb8//sdGBhoTp06leW0rP5PDh8+3NFmt9tN586djZeXlzl9+rQxxpg1a9YYSWbNmjW5LjO72ozJvM1HREQYLy8vc/jwYUfb8ePHTYkSJUyLFi0cben/N9q1a+e0PT3//PPG09PTnD9/Psv1oXBwmgOWzZgxQytXrnR65ObZZ591et68eXMdOXLEqc3X19fx73PnzikxMVHNmzdXXFxcnmuNiIhw+muycePGatKkif773/9Kkk6cOKFdu3apX79+Kl26tKNf3bp11b59e0e/6w0dOtTp+fDhwyUpy755cf04XLhwQWfOnFHz5s2VnJysn3766aaXl5fXmNX7dfbsWSUlJWW7nl27dungwYPq2bOnzp49qzNnzujMmTO6dOmS2rZtq/Xr12f6Szm39SxcuFB2u109evRwLO/MmTMqX768qlWrpjVr1jjN7+3trf79+zu1LV++XMWLF9fTTz/taPPw8Mj0PkpSnz59dPz4caflzp07V76+vurevXu2rz0tLU0rVqxQRESEKlas6Gi/55571LFjx2znS1eyZEn98MMPOnjwYK59s9O9e3cFBwffcP9hw4Y5/m2z2TRs2DBduXJF33zzTZ5ryE1aWpq+/vprRURE6O6773a0V6hQQT179tTGjRszbWODBg1yOm3SvHlzpaWl6dixYwVWJ3LHaQ5Y1rhx42wvwMyKj49Ppp1cqVKldO7cOae2pUuXatKkSdq1a5dSU1Md7Rnv0b8Z1apVy9RWvXp1/ec//5Ekxw6pRo0amfrdc889WrFihS5duiR/f/9sl1mlShV5eHjk23dF/PDDD3rppZe0evXqTDvWxMTEm15eXl7j9R+I0u/vl/R7yAsMDMxyPekfhBlPH1wvMTHRsawbWc/BgwdljMnyfZSU6S6FO++8U15eXk5tx44dU4UKFTJdiFm1atVMy2vfvr0qVKiguXPnqm3btrLb7frkk0/UtWtXlShRItvXdfr0aV2+fDnLOmvUqJFr0JwwYYK6du2q6tWrq3bt2urUqZN69+6tunXr5jjf9TLeYZUTDw8Ppw9z6ff/F5Ly9TtPMjp9+rSSk5Oz3RbtdrsSEhJUq1YtR3tO2whchzCBQufp6Zlrnw0bNuiRRx5RixYt9M4776hChQoqXry4YmNjNW/evEKoMu8yhp3swk/GC06zcv78ebVs2VKBgYGaMGGCqlSpIh8fH8XFxWnUqFHZngPPb9m9Z+a6C/QySq/tjTfeUP369bPsk/G6jtzWY7fbZbPZtGzZsiz7Zlze9Ud18sLT01M9e/bUrFmz9M4772jTpk06fvy4nnzySUvLzU2LFi10+PBhLVmyRF9//bVmz56tadOmaebMmRo4cOANLcPqa8/Iynacn/KyLaLgESbglj7//HP5+PhoxYoVTrfNxcbGWlpuVoeNDxw44LjgMCwsTNLv9/1n9NNPP+mOO+5w+os9fZnX/xV46NAh2e12xzLT/3I6f/6803w3clh27dq1Onv2rBYuXKgWLVo42uPj4zP1vdEjNnl5jXlRpUoVSVJgYKDatWtneXnpyzTGqHLlyo6/nG9WWFiY1qxZk+k20UOHDmXZv0+fPnrzzTf15ZdfatmyZQoODs71VEVwcLB8fX2z3N6yGveslC5dWv3791f//v118eJFtWjRQuPGjXOECStH6DKy2+06cuSI05geOHBAkvK0Hd9obcHBwfLz88t2W/Tw8FBoaOgNLQuuxTUTcEuenp6y2WxOf/UcPXpUixcvtrTcxYsXO93Ot23bNm3dulXh4eGSfj9XW79+fX300UdOO829e/fq66+/1p/+9KdMy0y/DS7d9OnTJcmxzMDAQN1xxx1av369U7933nkn13rT/wq7/q+uK1euZDmvv7//DZ32yMtrzIuGDRuqSpUqmjJlii5evJhpel5u5+vWrZs8PT01fvz4TH+JGmN09uzZXJfRsWNHXb16VbNmzXK02e32TO9jurp166pu3bqaPXu2Pv/8cz3++OO5fumSp6enOnbsqMWLF+vnn392tP/4449asWJFrjVmfB0BAQGqWrWq0+m+9MCX8cM9r95++23Hv40xevvtt1W8eHG1bdtW0u8hzNPT84a24xutzdPTUx06dNCSJUucTqf8+uuvmjdvnh566KFsT6PBvXBkAm6pc+fOmjp1qjp16qSePXvq1KlTmjFjhqpWrao9e/bkeblVq1bVQw89pMGDBys1NVUxMTEqU6aMXnzxRUefN954Q+Hh4WratKkGDBjguG0yKCgoy++EiI+P1yOPPKJOnTpp8+bN+ve//62ePXuqXr16jj4DBw7Uq6++qoEDB+r+++/X+vXrHX/55eTBBx9UqVKl1LdvX40YMUI2m03/+te/sjyk27BhQy1YsECRkZFq1KiRAgIC1KVLlyyXe7OvMS88PDw0e/ZshYeHq1atWurfv7/uvPNO/fLLL1qzZo0CAwP15Zdf3tQyq1SpokmTJikqKkpHjx5VRESESpQoofj4eC1atEiDBg3SX//61xyXERERocaNG+uFF17QoUOHVLNmTX3xxRf67bffJGX9V3WfPn0cy73RUxzjx4/X8uXL1bx5cw0ZMkTXrl3T9OnTVatWrVy34XvvvVetWrVSw4YNVbp0aW3fvl2fffaZ00WSDRs2lCSNGDFCHTt2lKenpx5//PEbqi0jHx8fLV++XH379lWTJk20bNkyffXVV/r73//uuL4pKChIjz32mKZPny6bzaYqVapo6dKlOnXqVKbl3UxtkyZN0sqVK/XQQw9pyJAhKlasmN577z2lpqbq9ddfz9PrgQu46jYS3PrSb9X67rvvspye3W1o/v7+mfqm3xp4vQ8++MBUq1bNeHt7m5o1a5rY2Ngs+93MraFvvPGGefPNN01oaKjx9vY2zZs3d9zCeb1vvvnGNGvWzPj6+prAwEDTpUsXs2/fvixr3rdvn/nzn/9sSpQoYUqVKmWGDRtmLl++7NQ3OTnZDBgwwAQFBZkSJUqYHj16mFOnTt3QraGbNm0yDzzwgPH19TUhISHmxRdfNCtWrMh0m97FixdNz549TcmSJY0kx22i2d2aejOvMf32wJzqzM7OnTtNt27dTJkyZYy3t7cJCwszPXr0MKtWrcrzej7//HPz0EMPGX9/f+Pv729q1qxphg4davbv3+/o07JlS1OrVq0sazp9+rTp2bOnKVGihAkKCjL9+vUzmzZtMpLM/PnzM/U/ceKE8fT0NNWrV8/19V5v3bp1pmHDhsbLy8vcfffdZubMmTe0DU+aNMk0btzYlCxZ0vj6+pqaNWuayZMnmytXrjj6XLt2zQwfPtwEBwcbm82W6ZbsN954I1M9Of2fPHz4sOnQoYPx8/Mz5cqVM2PHjnW6JTZ93Lp37278/PxMqVKlzDPPPGP27t2baZnZ1WZM5ltDjfn9dtuOHTuagIAA4+fnZ1q3bm2+/fZbpz7Z7W+yu2UVhctmDFet4PZ39OhRVa5cWW+88Uauf7neqHHjxmn8+PE6ffp0jl/ShVvD4sWL9eijj2rjxo1q1qyZ07QzZ86oQoUKevnllzVmzBgXVQi4L66ZAFDkXL582el5Wlqapk+frsDAQDVo0CBT/zlz5igtLU29e/curBKBWwrXTAAocoYPH67Lly+radOmSk1N1cKFC/Xtt9/qlVdecbqlcvXq1dq3b58mT56siIiITF8zDuB3hAkARU6bNm305ptvaunSpUpJSVHVqlU1ffp0pwscpd+/POrbb79Vs2bNHHfpAMiMayYAAIAlXDMBAAAsIUwAAABLbvtrJux2u44fP64SJUrk69fPAgBwuzPG6MKFCwoJCZGHR/bHH277MHH8+HG+2x0AAAsSEhJ01113ZTv9tg8T6T8TnJCQwHe8AwBwE5KSkhQaGur4LM3ObR8m0k9tBAYGEiYAAMiD3C4T4AJMAABgCWECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAltz2v80BAO6s0uiv8n2ZR1/tnO/LBHLCkQkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJa4NEysX79eXbp0UUhIiGw2mxYvXpxt32effVY2m00xMTGFVh8AAMidS8PEpUuXVK9ePc2YMSPHfosWLdKWLVsUEhJSSJUBAIAbVcyVKw8PD1d4eHiOfX755RcNHz5cK1asUOfOnQupMgAAcKNcGiZyY7fb1bt3b/3tb39TrVq1bmie1NRUpaamOp4nJSUVVHkAAEBufgHma6+9pmLFimnEiBE3PE90dLSCgoIcj9DQ0AKsEAAAuG2Y2LFjh9566y3NmTNHNpvthueLiopSYmKi45GQkFCAVQIAALcNExs2bNCpU6dUsWJFFStWTMWKFdOxY8f0wgsvqFKlStnO5+3trcDAQKcHAAAoOG57zUTv3r3Vrl07p7aOHTuqd+/e6t+/v4uqAgAAGbk0TFy8eFGHDh1yPI+Pj9euXbtUunRpVaxYUWXKlHHqX7x4cZUvX141atQo7FIBAEA2XBomtm/frtatWzueR0ZGSpL69u2rOXPmuKgqAABwM1waJlq1aiVjzA33P3r0aMEVAwAA8sRtL8AEAAC3BsIEAACwhDABAAAsIUwAAABLCBMAAMASwgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDABAAAsIUwAAABLCBMAAMASwgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDABAAAsIUwAAABLXBom1q9fry5duigkJEQ2m02LFy92TLt69apGjRqlOnXqyN/fXyEhIerTp4+OHz/uuoIBAEAmLg0Tly5dUr169TRjxoxM05KTkxUXF6cxY8YoLi5OCxcu1P79+/XII4+4oFIAAJCdYq5ceXh4uMLDw7OcFhQUpJUrVzq1vf3222rcuLF+/vlnVaxYsTBKBAAAuXBpmLhZiYmJstlsKlmyZLZ9UlNTlZqa6nielJRUCJUBAFB03TIXYKakpGjUqFF64oknFBgYmG2/6OhoBQUFOR6hoaGFWCUAAEXPLREmrl69qh49esgYo3fffTfHvlFRUUpMTHQ8EhISCqlKAACKJrc/zZEeJI4dO6bVq1fneFRCkry9veXt7V1I1QEAALcOE+lB4uDBg1qzZo3KlCnj6pIAAEAGLg0TFy9e1KFDhxzP4+PjtWvXLpUuXVoVKlTQn//8Z8XFxWnp0qVKS0vTyZMnJUmlS5eWl5eXq8oGAADXcWmY2L59u1q3bu14HhkZKUnq27evxo0bpy+++EKSVL9+faf51qxZo1atWhVWmQAAIAcuDROtWrWSMSbb6TlNAwAA7uGWuJsDAAC4L8IEAACwhDABAAAsIUwAAABLCBMAAMASwgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDABAAAsIUwAAABLCBMAAMASwgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDABAAAsIUwAAABLXBom1q9fry5duigkJEQ2m02LFy92mm6M0csvv6wKFSrI19dX7dq108GDB11TLAAAyJJLw8SlS5dUr149zZgxI8vpr7/+uv75z39q5syZ2rp1q/z9/dWxY0elpKQUcqUAACA7xVy58vDwcIWHh2c5zRijmJgYvfTSS+ratask6eOPP1a5cuW0ePFiPf7444VZKgAAyIbbXjMRHx+vkydPql27do62oKAgNWnSRJs3b852vtTUVCUlJTk9AABAwXHpkYmcnDx5UpJUrlw5p/Zy5co5pmUlOjpa48ePL9DaJKnS6K/yfZlHX+2c78sEAKCgue2RibyKiopSYmKi45GQkODqkgAAuK25bZgoX768JOnXX391av/1118d07Li7e2twMBApwcAACg4bhsmKleurPLly2vVqlWOtqSkJG3dulVNmzZ1YWUAAOB6Lr1m4uLFizp06JDjeXx8vHbt2qXSpUurYsWKGjlypCZNmqRq1aqpcuXKGjNmjEJCQhQREeG6ogEAgBOXhont27erdevWjueRkZGSpL59+2rOnDl68cUXdenSJQ0aNEjnz5/XQw89pOXLl8vHx8dVJQMAgAxcGiZatWolY0y20202myZMmKAJEyYUYlUAAOBmuO01EwAA4NZAmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAluQpTLRp00bnz5/P1J6UlKQ2bdpYrQkAANxC8hQm1q5dqytXrmRqT0lJ0YYNGywXBQAAbh3Fbqbznj17HP/et2+fTp486Xielpam5cuX684778y/6gAAgNu7qTBRv3592Ww22Wy2LE9n+Pr6avr06flWHAAAcH83FSbi4+NljNHdd9+tbdu2KTg42DHNy8tLZcuWlaenZ74XCQAA3NdNhYmwsDBJkt1uL5BiAADAreemwsT1Dh48qDVr1ujUqVOZwsXLL79suTAAAHBryFOYmDVrlgYPHqw77rhD5cuXl81mc0yz2WyECQAAipA8hYlJkyZp8uTJGjVqVH7XAwAAbjF5+p6Jc+fO6bHHHsvvWgAAwC0oT2Hiscce09dff53ftWSSlpamMWPGqHLlyvL19VWVKlU0ceJEGWMKfN0AAODG5Ok0R9WqVTVmzBht2bJFderUUfHixZ2mjxgxIl+Ke+211/Tuu+/qo48+Uq1atbR9+3b1799fQUFB+bYOAABgTZ7CxPvvv6+AgACtW7dO69atc5pms9ny7YP+22+/VdeuXdW5c2dJUqVKlfTJJ59o27Zt+bJ8AABgXZ7CRHx8fH7XkaUHH3xQ77//vg4cOKDq1atr9+7d2rhxo6ZOnZrtPKmpqUpNTXU8T0pKKoxSAQAosvL8PROFYfTo0UpKSlLNmjXl6emptLQ0TZ48Wb169cp2nujoaI0fP74QqwQAoGjLU5h46qmncpz+4Ycf5qmYjP7zn/9o7ty5mjdvnmrVqqVdu3Zp5MiRCgkJUd++fbOcJyoqSpGRkY7nSUlJCg0NzZd6AABAZnkKE+fOnXN6fvXqVe3du1fnz5/P8gfA8upvf/ubRo8erccff1ySVKdOHR07dkzR0dHZhglvb295e3vnWw0AACBneQoTixYtytRmt9s1ePBgValSxXJR6ZKTk+Xh4Xz3qqenJ78NAgCAG8nT90xkuSAPD0VGRmratGn5tUh16dJFkydP1ldffaWjR49q0aJFmjp1qh599NF8WwcAALAmXy/APHz4sK5du5Zvy5s+fbrGjBmjIUOG6NSpUwoJCdEzzzzDb38AAOBG8hQmrr/AUZKMMTpx4oS++uqrbK9lyIsSJUooJiZGMTEx+bZMAACQv/IUJnbu3On03MPDQ8HBwXrzzTdzvdMDAADcXvIUJtasWZPfdQAAgFuUpWsmTp8+rf3790uSatSooeDg4HwpCgAA3DrydDfHpUuX9NRTT6lChQpq0aKFWrRooZCQEA0YMEDJycn5XSMAAHBjeQoTkZGRWrdunb788kudP39e58+f15IlS7Ru3Tq98MIL+V0jAABwY3k6zfH555/rs88+U6tWrRxtf/rTn+Tr66sePXro3Xffza/6AACAm8vTkYnk5GSVK1cuU3vZsmU5zQEAQBGTpzDRtGlTjR07VikpKY62y5cva/z48WratGm+FQcAANxfnk5zxMTEqFOnTrrrrrtUr149SdLu3bvl7e2tr7/+Ol8LBAAA7i1PYaJOnTo6ePCg5s6dq59++kmS9MQTT6hXr17y9fXN1wIBAIB7y1OYiI6OVrly5fT00087tX/44Yc6ffq0Ro0alS/FAQAA95enaybee+891axZM1N7rVq1NHPmTMtFAQCAW0eewsTJkydVoUKFTO3BwcE6ceKE5aIAAMCtI09hIjQ0VJs2bcrUvmnTJoWEhFguCgAA3DrydM3E008/rZEjR+rq1atq06aNJGnVqlV68cUX+QZMAACKmDyFib/97W86e/ashgwZoitXrkiSfHx8NGrUKEVFReVrgQAAwL3lKUzYbDa99tprGjNmjH788Uf5+vqqWrVq8vb2zu/6AACAm7P0E+QBAQFq1KhRftUCAABuQXm6ABMAACAdYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABY4vZh4pdfftGTTz6pMmXKyNfXV3Xq1NH27dtdXRYAAPiDpd/mKGjnzp1Ts2bN1Lp1ay1btkzBwcE6ePCgSpUq5erSAADAH9w6TLz22msKDQ1VbGyso61y5courAgAAGTk1qc5vvjiC91///167LHHVLZsWd13332aNWtWjvOkpqYqKSnJ6QEAAAqOW4eJI0eO6N1331W1atW0YsUKDR48WCNGjNBHH32U7TzR0dEKCgpyPEJDQwuxYgAAih63DhN2u10NGjTQK6+8ovvuu0+DBg3S008/rZkzZ2Y7T1RUlBITEx2PhISEQqwYAICix63DRIUKFXTvvfc6td1zzz36+eefs53H29tbgYGBTg8AAFBw3DpMNGvWTPv373dqO3DggMLCwlxUEQAAyMitw8Tzzz+vLVu26JVXXtGhQ4c0b948vf/++xo6dKirSwMAAH9w6zDRqFEjLVq0SJ988olq166tiRMnKiYmRr169XJ1aQAA4A9u/T0TkvTwww/r4YcfdnUZAAAgG259ZAIAALg/wgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDABAAAsIUwAAABLCBMAAMASt//VUAAAbkeVRn+V78s8+mrnfF/mjeDIBAAAsIQwAQAALCFMAAAASwgTAADAEsIEAACwhDABAAAsIUwAAABLCBMAAMASwgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALCFMAAAAS26pMPHqq6/KZrNp5MiRri4FAAD84ZYJE999953ee+891a1b19WlAACA69wSYeLixYvq1auXZs2apVKlSrm6HAAAcJ1bIkwMHTpUnTt3Vrt27XLtm5qaqqSkJKcHAAAoOMVcXUBu5s+fr7i4OH333Xc31D86Olrjx48v4KoAAEA6tz4ykZCQoOeee05z586Vj4/PDc0TFRWlxMRExyMhIaGAqwQAoGhz6yMTO3bs0KlTp9SgQQNHW1pamtavX6+3335bqamp8vT0dJrH29tb3t7ehV0qAABFlluHibZt2+r77793auvfv79q1qypUaNGZQoSAACg8Ll1mChRooRq167t1Obv768yZcpkagcAAK7h1tdMAAAA9+fWRyaysnbtWleXAAAArsORCQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJW4dJqKjo9WoUSOVKFFCZcuWVUREhPbv3+/qsgAAwHXcOkysW7dOQ4cO1ZYtW7Ry5UpdvXpVHTp00KVLl1xdGgAA+EMxVxeQk+XLlzs9nzNnjsqWLasdO3aoRYsWLqoKAABcz63DREaJiYmSpNKlS2fbJzU1VampqY7nSUlJBV4XAABFmVuf5rie3W7XyJEj1axZM9WuXTvbftHR0QoKCnI8QkNDC7FKAACKnlsmTAwdOlR79+7V/Pnzc+wXFRWlxMRExyMhIaGQKgQAoGi6JU5zDBs2TEuXLtX69et111135djX29tb3t7ehVQZAABw6zBhjNHw4cO1aNEirV27VpUrV3Z1SQAAIAO3DhNDhw7VvHnztGTJEpUoUUInT56UJAUFBcnX19fF1QEAAMnNr5l49913lZiYqFatWqlChQqOx4IFC1xdGgAA+INbH5kwxri6BAAAkAu3PjIBAADcH2ECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABYQpgAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmAACAJYQJAABgCWECAABYQpgAAACWECYAAIAlt0SYmDFjhipVqiQfHx81adJE27Ztc3VJAADgD24fJhYsWKDIyEiNHTtWcXFxqlevnjp27KhTp065ujQAAKBbIExMnTpVTz/9tPr37697771XM2fOlJ+fnz788ENXlwYAACQVc3UBObly5Yp27NihqKgoR5uHh4fatWunzZs3ZzlPamqqUlNTHc8TExMlSUlJSflamz01OV+XJ+V/jQDcH/uSoutWeO/Tl2eMybGfW4eJM2fOKC0tTeXKlXNqL1eunH766acs54mOjtb48eMztYeGhhZIjfkpKMbVFQC4HbAvKboK6r2/cOGCgoKCsp3u1mEiL6KiohQZGel4brfb9dtvv6lMmTKy2Wz5so6kpCSFhoYqISFBgYGB+bLM2wHjkj3GJmuMS/YYm6wxLtkriLExxujChQsKCQnJsZ9bh4k77rhDnp6e+vXXX53af/31V5UvXz7Leby9veXt7e3UVrJkyQKpLzAwkI05C4xL9hibrDEu2WNsssa4ZC+/xyanIxLp3PoCTC8vLzVs2FCrVq1ytNntdq1atUpNmzZ1YWUAACCdWx+ZkKTIyEj17dtX999/vxo3bqyYmBhdunRJ/fv3d3VpAABAt0CY+Mtf/qLTp0/r5Zdf1smTJ1W/fn0tX74800WZhcnb21tjx47NdDqlqGNcssfYZI1xyR5jkzXGJXuuHBubye1+DwAAgBy49TUTAADA/REmAACAJYQJAABgCWECAABYQpjIRnR0tBo1aqQSJUqobNmyioiI0P79+zP127x5s9q0aSN/f38FBgaqRYsWunz5sgsqLjw3MjYnT55U7969Vb58efn7+6tBgwb6/PPPXVRx4Xj33XdVt25dxxfGNG3aVMuWLXNMT0lJ0dChQ1WmTBkFBASoe/fumb6Q7XaV09j89ttvGj58uGrUqCFfX19VrFhRI0aMcPyuzu0st20mnTFG4eHhstlsWrx4ceEX6gI3MjZFcf+b27i4at9LmMjGunXrNHToUG3ZskUrV67U1atX1aFDB126dMnRZ/PmzerUqZM6dOigbdu26bvvvtOwYcPk4XF7D+uNjE2fPn20f/9+ffHFF/r+++/VrVs39ejRQzt37nRh5QXrrrvu0quvvqodO3Zo+/btatOmjbp27aoffvhBkvT888/ryy+/1Keffqp169bp+PHj6tatm4urLhw5jc3x48d1/PhxTZkyRXv37tWcOXO0fPlyDRgwwNVlF7jctpl0MTEx+fZzALeK3MamqO5/cxsXl+17DW7IqVOnjCSzbt06R1uTJk3MSy+95MKq3ENWY+Pv728+/vhjp36lS5c2s2bNKuzyXKpUqVJm9uzZ5vz586Z48eLm008/dUz78ccfjSSzefNmF1boOuljk5X//Oc/xsvLy1y9erWQq3K9jOOyc+dOc+edd5oTJ04YSWbRokWuK87Frh8b9r//7/pxcdW+9/aOcPko/ZBr6dKlJUmnTp3S1q1bVbZsWT344IMqV66cWrZsqY0bN7qyTJfIODaS9OCDD2rBggX67bffZLfbNX/+fKWkpKhVq1YuqrJwpaWlaf78+bp06ZKaNm2qHTt26OrVq2rXrp2jT82aNVWxYkVt3rzZhZUWvoxjk5XExEQFBgaqWDG3/169fJPVuCQnJ6tnz56aMWNGtr9HVBRkHBv2v7/Laptx2b63QKPKbSItLc107tzZNGvWzNG2efNmI8mULl3afPjhhyYuLs6MHDnSeHl5mQMHDriw2sKV1dgYY8y5c+dMhw4djCRTrFgxExgYaFasWOGiKgvPnj17jL+/v/H09DRBQUHmq6++MsYYM3fuXOPl5ZWpf6NGjcyLL75Y2GW6RHZjk9Hp06dNxYoVzd///vdCrtA1chqXQYMGmQEDBjieq4gdmchubIr6/jenbcZV+17CxA149tlnTVhYmElISHC0bdq0yUgyUVFRTn3r1KljRo8eXdglukxWY2OMMcOGDTONGzc233zzjdm1a5cZN26cCQoKMnv27HFRpYUjNTXVHDx40Gzfvt2MHj3a3HHHHeaHH34gTJjsx+Z6iYmJpnHjxqZTp07mypUrLqq0cGU3LkuWLDFVq1Y1Fy5ccPQtamEiu7Ep6vvfnP4vuWrfS5jIxdChQ81dd91ljhw54tR+5MgRI8n861//cmrv0aOH6dmzZ2GW6DLZjc2hQ4eMJLN3716n9rZt25pnnnmmMEt0ubZt25pBgwaZVatWGUnm3LlzTtMrVqxopk6d6priXCx9bNIlJSWZpk2bmrZt25rLly+7sDLXSh+X5557zthsNuPp6el4SDIeHh6mZcuWri7TJdLHhv2vs/RxceW+l2smsmGM0bBhw7Ro0SKtXr1alStXdppeqVIlhYSEZLol8sCBAwoLCyvMUgtdbmOTnJwsSZmuqvb09JTdbi+0Ot2B3W5XamqqGjZsqOLFi2vVqlWOafv379fPP/+c7XUDt7v0sZGkpKQkdejQQV5eXvriiy/k4+Pj4upcJ31cRo8erT179mjXrl2OhyRNmzZNsbGxri3SRdLHpijvf7OSPi4u3fcWaFS5hQ0ePNgEBQWZtWvXmhMnTjgeycnJjj7Tpk0zgYGB5tNPPzUHDx40L730kvHx8TGHDh1yYeUFL7exuXLliqlatapp3ry52bp1qzl06JCZMmWKsdls2Z4nvx2MHj3arFu3zsTHx5s9e/aY0aNHG5vNZr7++mtjzO+nhCpWrGhWr15ttm/fbpo2bWqaNm3q4qoLR05jk5iYaJo0aWLq1KljDh065LRNXbt2zdWlF6jctpmMVIROc+Q2NkV1/5vTuLhy30uYyIakLB+xsbFO/aKjo81dd91l/Pz8TNOmTc2GDRtcU3AhupGxOXDggOnWrZspW7as8fPzM3Xr1s10u9Lt5qmnnjJhYWHGy8vLBAcHm7Zt2zp9KFy+fNkMGTLElCpVyvj5+ZlHH33UnDhxwoUVF56cxmbNmjXZblPx8fGuLbyA5bbNZFSUwsSNjE1R3P/mNi6u2vfyE+QAAMASrpkAAACWECYAAIAlhAkAAGAJYQIAAFhCmAAAAJYQJgAAgCWECQAAYAlhAgAAWEKYAAAAlhAmgNtQv379FBERkWu///3vf/Ly8lLt2rWznG6z2RyPYsWKqWLFioqMjHT8QJcknT59WoMHD1bFihXl7e2t8uXLq2PHjtq0aVN+vRwAbq6YqwsA4Dpz5sxRjx49tH79em3dulVNmjTJ1Cc2NladOnXS1atXtXv3bvXv31/+/v6aOHGiJKl79+66cuWKPvroI91999369ddftWrVKp09e7awX06urly5Ii8vL1eXAdx2ODIBFFHGGMXGxqp3797q2bOnPvjggyz7lSxZUuXLl1doaKgefvhhde3aVXFxcZKk8+fPa8OGDXrttdfUunVrhYWFqXHjxoqKitIjjzyS4/pnz56te+65Rz4+PqpZs6beeecdx7SjR4/KZrNp4cKFat26tfz8/FSvXj1t3rzZaRkbN25U8+bN5evrq9DQUI0YMUKXLl1yTK9UqZImTpyoPn36KDAwUIMGDZIkzZo1S6GhofLz89Ojjz6qqVOnqmTJko51e3h4aPv27U7riomJUVhYWMH/lDNwCyJMAEXUmjVrlJycrHbt2unJJ5/U/PnznT6Is3LgwAGtXr3acQQjICBAAQEBWrx4sdOpj9zMnTtXL7/8siZPnqwff/xRr7zyisaMGaOPPvrIqd8//vEP/fWvf9WuXbtUvXp1PfHEE7p27Zok6fDhw+rUqZO6d++uPXv2aMGCBdq4caOGDRvmtIwpU6aoXr162rlzp8aMGaNNmzbp2Wef1XPPPaddu3apffv2mjx5sqN/pUqV1K5dO8XGxjotJzY2Vv369ZOHB7tNIJMC/11SAIWub9++pmvXrjn26dmzpxk5cqTjeb169Zx+Rt6Y33/y2sfHx/j7+xtvb28jyTz88MPmypUrjj6fffaZKVWqlPHx8TEPPvigiYqKMrt3785x3VWqVDHz5s1zaps4caJp2rSpMcaY+Ph4I8nMnj3bMf2HH34wksyPP/5ojDFmwIABZtCgQU7L2LBhg/Hw8DCXL182xhgTFhZmIiIinPr85S9/MZ07d3Zq69WrlwkKCnI8X7BggSlVqpRJSUkxxhizY8cOY7PZbvufRAfyiogNFEHnz5/XwoUL9eSTTzrannzyySxPdUybNk27du3S7t27tXTpUh04cEC9e/d2TO/evbuOHz+uL774Qp06ddLatWvVoEEDzZkzJ8t1X7p0SYcPH9aAAQMcRzYCAgI0adIkHT582Klv3bp1Hf+uUKGCJOnUqVOSpN27d2vOnDlOy+jYsaPsdrvi4+Md891///1Oy9y/f78aN27s1JbxeUREhDw9PbVo0SJJv19b0rp1a1WqVCnL1wQUdVyACRRB8+bNU0pKitMFl8YY2e12HThwQNWrV3e0ly9fXlWrVpUk1ahRQxcuXNATTzyhSZMmOdp9fHzUvn17tW/fXmPGjNHAgQM1duxY9evXL9O6L168KOn36xYyXvDp6enp9Lx48eKOf9tsNklyXLNw8eJFPfPMMxoxYkSmdVSsWNHxb39//9wHJAMvLy/16dNHsbGx6tatm+bNm6e33nrrppcDFBWECaAI+uCDD/TCCy9k+rAfMmSIPvzwQ7366qvZzpv+gX/58uVs+9x7771avHhxltPKlSunkJAQHTlyRL169brp2tM1aNBA+/btcwSaG1WjRg199913Tm0Zn0vSwIEDVbt2bb3zzju6du2aunXrludagdsdYQK4TSUmJmrXrl1ObWXKlNHZs2cVFxenuXPnqmbNmk7Tn3jiCU2YMEGTJk1SsWK/7x7Onz+vkydPym636+DBg5owYYKqV6+ue+65R2fPntVjjz2mp556SnXr1lWJEiW0fft2vf766+ratWu2tY0fP14jRoxQUFCQOnXqpNTUVG3fvl3nzp1TZGTkDb2+UaNG6YEHHtCwYcM0cOBA+fv7a9++fVq5cqXefvvtbOcbPny4WrRooalTp6pLly5avXq1li1b5jjyke6ee+7RAw88oFGjRumpp56Sr6/vDdUFFEmuvmgDQP7r27evkZTpMWDAADNs2DBz7733ZjnfiRMnjIeHh1myZIkxxjjNa7PZTIUKFcxf/vIXc/jwYWOMMSkpKWb06NGmQYMGJigoyPj5+ZkaNWqYl156ySQnJ+dY49y5c039+vWNl5eXKVWqlGnRooVZuHChMeb/L8DcuXOno/+5c+eMJLNmzRpH27Zt20z79u1NQECA8ff3N3Xr1jWTJ092TA8LCzPTpk3LtO7333/f3HnnncbX19dERESYSZMmmfLly2fq98EHHxhJZtu2bTm+FqCosxljjItyDAC4haefflo//fSTNmzY4NQ+ceJEffrpp9qzZ4+LKgNuDZzmAFDkTJkyRe3bt5e/v7+WLVumjz76yOlLsy5evKijR4/q7bff1qRJk1xYKXBr4MgEgCKnR48eWrt2rS5cuKC7775bw4cP17PPPuuY3q9fP33yySeKiIjQvHnzMt1lAsAZYQIAAFjCl1YBAABLCBMAAMASwgQAALCEMAEAACwhTAAAAEsIEwAAwBLCBAAAsIQwAQAALPk/I3hGkOlngJMAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO - write code here to sample from your CUDA-Q kernel and used the results to seed your MTS population\n",
        "def bitstring_to_pm1(bs: str) -> np.ndarray:\n",
        "    # cudaq returns bitstrings like \"0101...\" (convention doesn't matter for LABS energy)\n",
        "    arr01 = np.array([int(c) for c in bs], dtype=int)\n",
        "    return 2 * arr01 - 1\n",
        "\n",
        "def build_population_from_counts(counts, pop_size: int) -> np.ndarray:\n",
        "    # counts supports iteration: for bitstring, count in counts.items()\n",
        "    pop = []\n",
        "    for bs, c in counts.items():\n",
        "        s = bitstring_to_pm1(bs)\n",
        "        for _ in range(int(c)):\n",
        "            pop.append(s.copy())\n",
        "            if len(pop) >= pop_size:\n",
        "                return np.stack(pop, axis=0)\n",
        "    # if not enough (rare), pad by resampling from what we have\n",
        "    if len(pop) == 0:\n",
        "        raise ValueError(\"No samples in counts; try increasing shots.\")\n",
        "    while len(pop) < pop_size:\n",
        "        pop.append(pop[len(pop) % len(pop)].copy())\n",
        "    return np.stack(pop, axis=0)\n",
        "\n",
        "def mts_with_initial_population(\n",
        "    init_pop: np.ndarray,\n",
        "    generations: int = 200,\n",
        "    p_mutate: float = 0.2,\n",
        "    p_bit: float = 0.05,\n",
        "    tabu_iters: int = 300,\n",
        "    tabu_tenure: int = 7,\n",
        "    seed: int = 0,\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    pop = init_pop.copy().astype(int)\n",
        "    pop_size, N = pop.shape\n",
        "    energies = np.array([labs_energy(ind) for ind in pop], dtype=int)\n",
        "\n",
        "    best_idx = int(np.argmin(energies))\n",
        "    best_s = pop[best_idx].copy()\n",
        "    best_E = int(energies[best_idx])\n",
        "\n",
        "    for _ in range(generations):\n",
        "        p1 = tournament_select(pop, energies, rng, k=3)\n",
        "        p2 = tournament_select(pop, energies, rng, k=3)\n",
        "\n",
        "        child = combine_uniform(p1, p2, rng)\n",
        "        if rng.random() < p_mutate:\n",
        "            child = mutate(child, rng, p_bit=p_bit)\n",
        "\n",
        "        improved, e_improved = tabu_search(child, max_iters=tabu_iters, tenure=tabu_tenure)\n",
        "\n",
        "        j = int(rng.integers(0, pop_size))\n",
        "        pop[j] = improved\n",
        "        energies[j] = e_improved\n",
        "\n",
        "        if e_improved < best_E:\n",
        "            best_E = e_improved\n",
        "            best_s = improved.copy()\n",
        "\n",
        "    return best_s, best_E, pop, energies\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Run CUDA-Q to get samples\n",
        "# ----------------------------\n",
        "T = 1\n",
        "n_steps = 1\n",
        "dt = T / n_steps\n",
        "N = 20\n",
        "\n",
        "G2, G4 = get_interactions(N)\n",
        "\n",
        "thetas = []\n",
        "for step in range(1, n_steps + 1):\n",
        "    t = step * dt\n",
        "    thetas.append(utils.compute_theta(t, dt, T, N, G2, G4))\n",
        "\n",
        "shots = 500  # increase for a better seeded population\n",
        "counts = cudaq.sample(trotterized_circuit, N, G2, G4, n_steps, dt, T, thetas, shots_count=shots)\n",
        "\n",
        "# Build initial population from the sample distribution\n",
        "pop_size = 30\n",
        "qe_init_pop = build_population_from_counts(counts, pop_size=pop_size)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Compare QE-init vs random-init\n",
        "# ----------------------------\n",
        "qe_best_s, qe_best_E, qe_pop, qe_energies = mts_with_initial_population(\n",
        "    qe_init_pop,\n",
        "    generations=200,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "rand_best_s, rand_best_E, rand_pop, rand_energies = mts(\n",
        "    N,\n",
        "    pop_size=pop_size,\n",
        "    generations=200,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "print(\"QE-MTS best_E:\", qe_best_E)\n",
        "print(\"Random MTS best_E:\", rand_best_E)\n",
        "\n",
        "# Optional: visualize final energy distributions\n",
        "plot_population_energies(qe_energies)\n",
        "plot_population_energies(rand_energies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5756e2-e4cb-42f4-a4b3-7f627095f4f4",
      "metadata": {},
      "source": [
        "The results clearly show that a population sampled from CUDA-Q results in an improved distribution and a lower energy final result. This is exactly the goal of quantum enhanced optimization.  To not necessarily solve the problem, but improve the effectiveness of state-of-the-art classical approaches. \n",
        "\n",
        "A few major caveats need to be mentioned here. First, We are comparing a quantum generated population to a random sample.  It is quite likely that other classical or quantum heuristics could be used to produce an initial population that might even beat the counteradiabatic method you used, so we cannot make any claims that this is the best. \n",
        "\n",
        "Recall that the point of the counteradiabatic approach derived in the paper is that it is more efficient in terms of two-qubit gates relative to QAOA. The benefits of this regime would only truly come into play in a setting (e.g. larger problem instance) where it is too difficult to produce a good initial population with any know classical heuristic, and the counteradiabatic approach is more efficiently run on a QPU compared to alternatives.\n",
        "\n",
        "We should also note that we are comparing a single sample of each approach.  Maybe the quantum sample got lucky or the randomly generated population was unlucky and a more rigorous comparison would need to repeat the analysis many times to draw any confidently conclusions.  \n",
        "\n",
        "The authors of the paper discuss all of these considerations, but propose an analysis that is quite interesting related to the scaling of the technique. Rather than run large simulations ourselves, examine their results below. \n",
        "\n",
        "\n",
        "<img src=\"images/quantum_enhanced_optimization_LABS/tabu_search_results.png\" width=\"900\">\n",
        "\n",
        "The authors computed replicate median (median of solving the problem repeated with same setup) time to solutions (excluding time to sample from QPU) for problem sizes $N=27$ to $N=37$. Two interesting conclusions can be drawn from this. First, standard memetic tabu search (MTS) is generally faster than quantum enhanced (QE) MTS.  But there are two promising trends. For larger problems, the QE-MTS experiments occasionally have excellent performance with times to solution much smaller than all of the MTS data points.  These outliers indicate there are certain instances where QE-MTS could provide much faster time-to-solution. \n",
        "\n",
        "More importantly, if a line of best fit is calculated using the median of each set of medians, the slope of the QE-MTS line is smaller than the MTS!  This seems to indicate that QE solution of this problem scales $O(1.24^N)$ which is better than the best known classical heuristic ($O(1.34^N)$) and the best known quantum approach (QAOA - $O(1.46^N)$).\n",
        "\n",
        "For problems of size of $N=47$ or greater, the authors anticipate that QE-MTS could be a promising technique and produce good initial populations that are difficult to obtain classically. \n",
        "\n",
        "The study reinforces the potential of hybrid workflows enhanced by quantum data such that a classical routine is still the primary solver, but quantum computers make it much more effective.  Future work can explore improvements to both the quantum and classical sides, such as including GPU accelerated memetic search on the classical side."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aab7af9",
      "metadata": {},
      "source": [
        "## Self-validation To Be Completed for Phase 1\n",
        "\n",
        "In this section, explain how you verified your results. Did you calculate solutions by hand for small N? Did you create unit tests? Did you cross-reference your Quantum energy values against your Classical MTS results? Did you check known symmetries?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c5777c39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All Phase 1 tests passed.\n"
          ]
        }
      ],
      "source": [
        "# Self-validation / tests (Phase 1)\n",
        "# Run this cell after executing the earlier cells that define:\n",
        "# - labs_energy, labs_correlations, delta_for_flip, tabu_search\n",
        "# - get_interactions\n",
        "#\n",
        "# These tests focus on *theoretical correctness* (identities/invariants),\n",
        "# not on whether the heuristic always finds the global optimum.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _assert(cond: bool, msg: str):\n",
        "    if not cond:\n",
        "        raise AssertionError(msg)\n",
        "\n",
        "\n",
        "def _energy_naive_pm1(s_pm1: np.ndarray) -> int:\n",
        "    s = np.asarray(s_pm1, dtype=int)\n",
        "    N = len(s)\n",
        "    E = 0\n",
        "    for k in range(1, N):\n",
        "        ck = 0\n",
        "        for i in range(0, N - k):\n",
        "            ck += int(s[i] * s[i + k])\n",
        "        E += ck * ck\n",
        "    return int(E)\n",
        "\n",
        "\n",
        "def _bruteforce_optimum_energy(N: int) -> int:\n",
        "    # brute force over {±1}^N (OK for small N only)\n",
        "    best = None\n",
        "    for mask in range(1 << N):\n",
        "        # 0/1 -> ±1\n",
        "        bits = np.array([(mask >> i) & 1 for i in range(N)], dtype=int)\n",
        "        s = 2 * bits - 1\n",
        "        e = _energy_naive_pm1(s)\n",
        "        if best is None or e < best:\n",
        "            best = e\n",
        "    return int(best)\n",
        "\n",
        "\n",
        "def test_energy_matches_naive():\n",
        "    rng = np.random.default_rng(0)\n",
        "    for N in range(2, 16):\n",
        "        for _ in range(50):\n",
        "            s = rng.choice([-1, 1], size=N)\n",
        "            e_fast = labs_energy(s)\n",
        "            e_naive = _energy_naive_pm1(s)\n",
        "            _assert(e_fast == e_naive, f\"labs_energy mismatch for N={N}: {e_fast} vs {e_naive}\")\n",
        "\n",
        "\n",
        "def test_energy_symmetries():\n",
        "    # LABS energy is invariant under global sign flip and reversal.\n",
        "    rng = np.random.default_rng(1)\n",
        "    for N in range(2, 32):\n",
        "        for _ in range(20):\n",
        "            s = rng.choice([-1, 1], size=N)\n",
        "            e = labs_energy(s)\n",
        "            _assert(labs_energy(-s) == e, f\"Energy not invariant under global sign flip (N={N})\")\n",
        "            _assert(labs_energy(s[::-1]) == e, f\"Energy not invariant under reversal (N={N})\")\n",
        "\n",
        "\n",
        "def test_delta_for_flip_is_exact():\n",
        "    # This is the most important correctness check for tabu_search speedups.\n",
        "    rng = np.random.default_rng(2)\n",
        "    for N in range(2, 40):\n",
        "        s = rng.choice([-1, 1], size=N)\n",
        "        C = labs_correlations(s)\n",
        "        E = int(np.dot(C[1:], C[1:]))\n",
        "        for i in range(N):\n",
        "            dE, dC = delta_for_flip(i, s, C)\n",
        "            s2 = s.copy(); s2[i] *= -1\n",
        "            C2 = labs_correlations(s2)\n",
        "            E2 = int(np.dot(C2[1:], C2[1:]))\n",
        "            _assert(int(E + dE) == int(E2), f\"dE wrong at N={N}, i={i}: {E}+{dE} vs {E2}\")\n",
        "            _assert(np.array_equal(C + dC, C2), f\"dC wrong at N={N}, i={i}\")\n",
        "\n",
        "\n",
        "def test_tabu_search_returns_consistent_energy():\n",
        "    rng = np.random.default_rng(3)\n",
        "    for N in range(2, 30):\n",
        "        s0 = rng.choice([-1, 1], size=N)\n",
        "        e0 = labs_energy(s0)\n",
        "        best_s, best_e = tabu_search(s0, max_iters=200, tenure=7)\n",
        "        _assert(best_e == labs_energy(best_s), f\"tabu_search energy inconsistent for N={N}\")\n",
        "        _assert(best_e <= e0, f\"tabu_search should not return worse than start (N={N})\")\n",
        "\n",
        "\n",
        "def test_get_interactions_counts_and_bounds():\n",
        "    for N in range(2, 40):\n",
        "        G2, G4 = get_interactions(N)\n",
        "\n",
        "        # counts implied directly by Eq. 15 loop limits\n",
        "        expected_g2 = sum((N - i0 - 1) // 2 for i0 in range(0, max(0, N - 2)))\n",
        "        expected_g4 = 0\n",
        "        for i0 in range(0, max(0, N - 3)):\n",
        "            t_max = (N - i0 - 2) // 2\n",
        "            for t in range(1, t_max + 1):\n",
        "                expected_g4 += (N - i0 - 2 * t - 1)\n",
        "\n",
        "        _assert(len(G2) == expected_g2, f\"G2 length mismatch for N={N}: {len(G2)} vs {expected_g2}\")\n",
        "        _assert(len(G4) == expected_g4, f\"G4 length mismatch for N={N}: {len(G4)} vs {expected_g4}\")\n",
        "\n",
        "        # bounds and basic structure\n",
        "        for (i, j) in G2:\n",
        "            _assert(0 <= i < N and 0 <= j < N, f\"G2 index out of bounds for N={N}: {(i,j)}\")\n",
        "            _assert(i < j, f\"G2 should have i<j for N={N}: {(i,j)}\")\n",
        "\n",
        "        for (a, b, c, d) in G4:\n",
        "            _assert(0 <= a < N and 0 <= b < N and 0 <= c < N and 0 <= d < N,\n",
        "                    f\"G4 index out of bounds for N={N}: {(a,b,c,d)}\")\n",
        "            _assert(a < b < c < d, f\"G4 should be strictly increasing for N={N}: {(a,b,c,d)}\")\n",
        "\n",
        "        # no duplicates\n",
        "        _assert(len(set(tuple(x) for x in G2)) == len(G2), f\"Duplicate in G2 for N={N}\")\n",
        "        _assert(len(set(tuple(x) for x in G4)) == len(G4), f\"Duplicate in G4 for N={N}\")\n",
        "\n",
        "\n",
        "def test_bruteforce_optimum_small_N():\n",
        "    # Verifies the objective definition against brute force (small N only).\n",
        "    # Known-optimum values are NOT hardcoded (avoids relying on memory/typos).\n",
        "    for N in range(2, 13):\n",
        "        opt = _bruteforce_optimum_energy(N)\n",
        "        _assert(isinstance(opt, int) and opt >= 0, f\"Invalid optimum energy for N={N}: {opt}\")\n",
        "\n",
        "\n",
        "def run_all_tests():\n",
        "    test_energy_matches_naive()\n",
        "    test_energy_symmetries()\n",
        "    test_delta_for_flip_is_exact()\n",
        "    test_tabu_search_returns_consistent_energy()\n",
        "    test_get_interactions_counts_and_bounds()\n",
        "    test_bruteforce_optimum_small_N()\n",
        "    print(\"All Phase 1 tests passed.\")\n",
        "\n",
        "# Uncomment to run everything:\n",
        "run_all_tests()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d24ff26",
      "metadata": {},
      "source": [
        "## Note on the \\(N=7\\) histogram (why a bar at 7 is expected)\n",
        "\n",
        "A quick brute-force sanity check over all \\(2^7 = 128\\) sequences confirms that the LABS objective for \\(N=7\\) can take multiple discrete values.\n",
        "\n",
        "- **The global optimum energy is \\(E=3\\)**.\n",
        "- **\\(E=7\\) is also a valid energy level** and occurs for many sequences.\n",
        "\n",
        "So, if your final population histogram shows a dominant bar at **3** and a smaller bar at **7**, that is consistent with the true objective landscape for \\(N=7\\).\n",
        "\n",
        "### Why MTS may still keep some \\(E=7\\) individuals in the *final population*\n",
        "\n",
        "In the provided `mts(...)` implementation, each generation produces one improved child (via tabu search) and then **replaces a random population member**:\n",
        "\n",
        "- This replacement rule is intentionally simple, but it does **not** guarantee the population becomes entirely optimal.\n",
        "- Even if the algorithm finds an \\(E=3\\) solution (and often keeps finding them), random replacement can leave a few non-optimal individuals (like \\(E=7\\)) in the population.\n",
        "\n",
        "This is not a correctness issue: **MTS is a heuristic optimizer**, and what matters most is that it can discover and track the best solutions, not that every member converges.\n",
        "\n",
        "*(If you want the final population to collapse more strongly to the best energies, a common tweak is replacing the **worst** individual instead of a random one, or only inserting the child if it beats the current worst.)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b558f818",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
